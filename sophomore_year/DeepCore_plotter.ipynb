{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c6895d",
   "metadata": {},
   "source": [
    "This notebook is used to make parameter plots, score plots. I usually run this on SWAN with 4 cores and 16 GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167f4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model,load_model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Reshape, Conv2DTranspose, concatenate, Concatenate, ZeroPadding2D, UpSampling2D, UpSampling1D\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate as concatenatenp\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "import matplotlib as mpl\n",
    "#mpl.use('Agg') # disables showing plots\n",
    "import matplotlib.backends.backend_pdf as backpdf\n",
    "from  matplotlib import pyplot as plt\n",
    "import pylab\n",
    "import glob\n",
    "import scipy\n",
    "\n",
    "import uproot\n",
    "\n",
    "#from IPython.display import Image\n",
    "#from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386c58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable init\n",
    "jetNum=0# number of jets in the input. will be filled with local input information\n",
    "jetNum_validation = 0# number of jets in the input. will be filled with local input information\n",
    "jetDim=30 #dimension of window on the pixed detector layer (cannot be changed without chaning the training sample)\n",
    "overlapNum =3 #numer of overlap considered (cannot be changed without chaning the training sample)\n",
    "layNum = 4 ## 4 for barrel, for endcap use layNum = 7 #4 barrel+3 endcap. the numeration is 1-4 for barrel, 5-7 for endcap (cannot be changed without chaning the training sample).\n",
    "parNum=5 #number of track parameters (cannot be changed without chaning the training sample)\n",
    "_Epsilon = 1e-7 #value needed for the loss functione valuation\n",
    "batch_size = 64 # Batch size for training.\n",
    "# DeepCore samples have the following file/tree names\n",
    "inputModuleName= \"DeepCoreNtuplizerTest\" \n",
    "inputTreeName= \"DeepCoreNtuplizerTree\" \n",
    "\n",
    "# Default prob_thr. Needs to be adjust depending on the DeepCore version used\n",
    "prob_thr =0.32 # threshold to identfy good prediciton (see DeepCore documentation to details)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d168c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class and function definition\n",
    "class validationCall(Callback) : \n",
    "    def on_epoch_end(self,epoch, logs={}) :\n",
    "        [call_par, call_prob] = self.model.predict([input_,input_jeta,input_jpt])\n",
    "        call_prob = call_prob[:,:,:,:,:-1]\n",
    "\n",
    "        for par in range(parNum) :\n",
    "            bins = []# np.zeros(shape=(int(jetNum*valSplit)))\n",
    "            nbin =0\n",
    "            for j in range (int(jetNum*valSplit)) :\n",
    "                j_eff = j+int(jetNum*(1-valSplit))\n",
    "                for x in range(jetDim) :\n",
    "                    for y in range(jetDim) :\n",
    "                        for trk in range(overlapNum) :\n",
    "                             if target_prob[j_eff][x][y][trk][0] == 1 :\n",
    "                                if(par!=4) :\n",
    "                                    bins.append((call_par[j_eff][x][y][trk][par] - target_[j_eff][x][y][trk][par])*0.01)\n",
    "                                else :\n",
    "                                     bins.append((call_par[j_eff][x][y][trk][par] - target_[j_eff][x][y][trk][par])/target_[j_eff][x][y][trk][par])  #relative\n",
    "                                nbin = nbin+1\n",
    "\n",
    "            plt.figure()\n",
    "            pylab.hist(bins,100, facecolor='green', alpha=0.75)\n",
    "            pylab.title('parNum error distribution_ep{EPOCH}_par{PAR}'.format(PAR=par,EPOCH=epoch))\n",
    "            pylab.ylabel('entries')\n",
    "            pylab.xlabel('parNum error')\n",
    "            plt.grid(True)\n",
    "            # pylab.savefig(\"parameter_error_{EPOCH}_{PAR}.pdf\".format(PAR=par,EPOCH=epoch))\n",
    "            pdf_par.savefig()\n",
    "\n",
    "        N_eff_4 = 0\n",
    "        N_eff_8 = 0\n",
    "        N_fake_4 =0\n",
    "        N_fake_8 = 0\n",
    "        genTrackNum=3\n",
    "        N_tot_eff = jetNum*valSplit*genTrackNum\n",
    "        N_tot_fake = 0\n",
    "        layDist=3\n",
    "        for j in range (int(jetNum*valSplit)) :\n",
    "            j_eff = j+int(jetNum*(1-valSplit))\n",
    "            for x in range(jetDim) :\n",
    "                for y in range(jetDim) :\n",
    "                    for trk in range(overlapNum) :\n",
    "                        if target_prob[j_eff][x][y][trk][0]==1 :\n",
    "                            chi2x = (call_par[j_eff][x][y][trk][0] - target_[j_eff][x][y][trk][0])**2\n",
    "                            chi2y = (call_par[j_eff][x][y][trk][1] - target_[j_eff][x][y][trk][1])**2\n",
    "                            chi2xt = (call_par[j_eff][x][y][trk][2] - target_[j_eff][x][y][trk][2])**2 / math.atan(2/float(layDist*3))\n",
    "                            chi2yt = (call_par[j_eff][x][y][trk][3] - target_[j_eff][x][y][trk][3])**2 / math.atan(2/float(layDist*3))\n",
    "                            chi2 = chi2x+chi2y+chi2xt+chi2yt\n",
    "                            if chi2<=4  and call_prob[j_eff][x][y][trk]>prob_thr:\n",
    "                                N_eff_4 = N_eff_4 +1\n",
    "                            if chi2<=8  and call_prob[j_eff][x][y][trk]>prob_thr:\n",
    "                                N_eff_8 = N_eff_8 +1\n",
    "                        if call_prob[j_eff][x][y][trk] > prob_thr :\n",
    "                            N_tot_fake = N_tot_fake +1\n",
    "                            chi2x = (call_par[j_eff][x][y][trk][0] - target_[j_eff][x][y][trk][0])**2\n",
    "                            chi2y = (call_par[j_eff][x][y][trk][1] - target_[j_eff][x][y][trk][1])**2\n",
    "                            chi2xt = (call_par[j_eff][x][y][trk][2] - target_[j_eff][x][y][trk][2])**2 / math.atan(2/float(layDist*3))\n",
    "                            chi2yt = (call_par[j_eff][x][y][trk][3] - target_[j_eff][x][y][trk][3])**2 / math.atan(2/float(layDist*3))\n",
    "                            chi2 = chi2x+chi2y+chi2xt+chi2yt\n",
    "                            if chi2>=4  and target_prob[j_eff][x][y][trk][0]==1:\n",
    "                                N_fake_4 = N_fake_4 +1\n",
    "                            if chi2>=8  and target_prob[j_eff][x][y][trk][0]==1:\n",
    "                                N_fake_8 = N_fake_8 +1\n",
    "\n",
    "        efficiency_4[epoch] = N_eff_4/N_tot_eff\n",
    "        if N_tot_fake == 0 :\n",
    "            fake_rate_4[epoch] = 1\n",
    "        else :\n",
    "           fake_rate_4[epoch] = N_fake_4/N_tot_fake\n",
    "\n",
    "        efficiency_8[epoch] = N_eff_8/N_tot_eff\n",
    "        if N_tot_fake == 0  :\n",
    "            fake_rate_8[epoch] = 1\n",
    "        else :\n",
    "           fake_rate_8[epoch] = N_fake_8/N_tot_fake\n",
    "\n",
    "#callback to have additional trained model every 10 epochs\n",
    "class wHistory(keras.callbacks.Callback):\n",
    "   def on_epoch_end(self, epoch, logs={}):\n",
    "       if epoch % 10 == 0 :\n",
    "               self.model.save(\"trained\"+str(epoch+0)+\".h5\")\n",
    "wH = wHistory()\n",
    "\n",
    "#callback to have the weight saved every batch\n",
    "class WeightsSaver(Callback):\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.batch = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if self.batch % self.N == 0:\n",
    "            name = 'weights%08d.h5' % self.batch\n",
    "            self.model.save_weights(name)\n",
    "        self.batch += 1\n",
    "\n",
    "#used in EXTRA_VALIDATION to have additional log info\n",
    "class NBatchLogger(Callback):\n",
    "    \"\"\"\n",
    "    A Logger that log average performance per `display` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print('step: {}/{} ... {}'.format(self.step,\n",
    "                                          self.params['steps'],\n",
    "                                          metrics_log))\n",
    "            self.metric_cache.clear()\n",
    "\n",
    "\n",
    "#used in the losses\n",
    "def _to_tensor(x, dtype):\n",
    "    return ops.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "#used in the losses\n",
    "def epsilon():\n",
    "    return _Epsilon\n",
    "\n",
    "#loss function for probability, used in the first part of the training\n",
    "def loss_ROI_crossentropy(target, output):\n",
    "    epsilon_ = _to_tensor(keras.backend.epsilon(), output.dtype.base_dtype)\n",
    "    output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
    "    wei = target[:,:,:,:,-1:]\n",
    "    target = target[:,:,:,:,:-1]\n",
    "    output = output[:,:,:,:,:-1]\n",
    "    output = math_ops.log(output / (1 - output))\n",
    "    retval = nn.weighted_cross_entropy_with_logits(targets=target, logits=output, pos_weight=10)#900=works #2900=200x200, 125=30x30\n",
    "    retval = retval*wei\n",
    "    return tf.reduce_sum(retval, axis=None)/(tf.reduce_sum(wei,axis=None)+0.00001) #0.00001 needed to avoid numeric issue\n",
    "    #return tf.reduce_sum(retval, axis=None)\n",
    "\n",
    "#loss function for probability, used in the last part of the training (difference: non-zero weight to pixel far from crossing point)\n",
    "def loss_ROIsoft_crossentropy(target, output):\n",
    "    epsilon_ = _to_tensor(keras.backend.epsilon(), output.dtype.base_dtype)\n",
    "    output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
    "    wei = target[:,:,:,:,-1:]\n",
    "    target = target[:,:,:,:,:-1]\n",
    "    output = output[:,:,:,:,:-1]\n",
    "    output = math_ops.log(output / (1 - output))\n",
    "    retval = nn.weighted_cross_entropy_with_logits(targets=target, logits=output, pos_weight=10)#900=works #2900=200x200, 125=30x30\n",
    "    retval = retval*(wei+0.01) # here the difference\n",
    "    return tf.reduce_sum(retval, axis=None)/(tf.reduce_sum((wei+0.01),axis=None))\n",
    "\n",
    "#loss for track parameter\n",
    "def loss_mse_select_clipped(y_true, y_pred) :\n",
    "    wei = y_true[:,:,:,:,-1:]\n",
    "    pred = y_pred[:,:,:,:,:-1]\n",
    "    true =  y_true[:,:,:,:,:-1]\n",
    "    print(wei)\n",
    "    print(pred)\n",
    "    out =K.square(tf.clip_by_value(pred-true,-5,5))*wei\n",
    "    return tf.reduce_sum(out, axis=None)/(tf.reduce_sum(wei,axis=None)*5+0.00001) #5=parNum\n",
    "\n",
    "# Generator used to load all the input file in the LOCAL_INPUT=False workflow\n",
    "## Changed uproot to uproot3 in order to use central sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94119163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated TP loss for DeepCore 2.2\n",
    "#loss for track parameter\n",
    "def loss_mse_select_clipped(y_true, y_pred) :\n",
    "    wei = y_true[:,:,:,:,-1:]\n",
    "    pred = y_pred[:,:,:,:,:-1]\n",
    "    true =  y_true[:,:,:,:,:-1]\n",
    "    inv_sd = tf.constant([1/0.404, 1/0.478, 1/1.9, 1/2, 1/150], dtype=tf.float32) # inverse standard deviation of each TP\n",
    "    mean = tf.constant([0, 0, 0, 0, 95], dtype=tf.float32) # mean of each TP\n",
    "    # Standardization of target and predicted TPs\n",
    "    pred = tf.subtract(pred, mean)* inv_sd\n",
    "    true = tf.subtract(true,mean)*inv_sd\n",
    "    out =K.square(tf.clip_by_value(pred-true,-5,5))*wei\n",
    "    return tf.reduce_sum(out, axis=None)/(tf.reduce_sum(wei,axis=None)*5+0.00001) #5=parNum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29591b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of Generator 2: used to load inputs (merged clusters) using less memory\n",
    "def Generator2(filepath,batch_size=0,count=False):\n",
    "\n",
    "    if count:\n",
    "        branches = [\"jet_eta\"]\n",
    "        batch_size = 500\n",
    "    else:\n",
    "        branches = [\"cluster_measured\",\"jet_eta\",\"jet_pt\",\"trackPar\",\"trackProb\"]\n",
    "\n",
    "    while 1:\n",
    "        for cycle in range(1,9):\n",
    "           # print(cycle)\n",
    "            for chunk in uproot.iterate(\"{}:{}/{};{}\".format(filepath,inputModuleName,inputTreeName,str(cycle)),branches,step_size=batch_size,library=\"np\"):\n",
    "                if count:\n",
    "                    yield chunk['jet_eta'].shape[0]\n",
    "                else:\n",
    "                    nev = len(chunk[\"trackProb\"])\n",
    "\n",
    "                    target_prob = np.reshape(chunk[\"trackProb\"], (nev,jetDim,jetDim,overlapNum,1))\n",
    "\n",
    "                    target_prob = concatenatenp([target_prob,chunk[\"trackPar\"][:,:,:,:,-1:]],axis=4)\n",
    "                    ## debug\n",
    "                    ##pdb.set_trace()\n",
    "                    \n",
    "                    \n",
    "                    yield [chunk['cluster_measured'][:,:,:,0:layNum],chunk[\"jet_eta\"],chunk[\"jet_pt\"]],[chunk[\"trackPar\"],target_prob]\n",
    "                    \n",
    "        if count:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c95c92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of  testing files= 30\n",
      "total number of events = 915878\n"
     ]
    }
   ],
   "source": [
    "# Testing samples info\n",
    "inputs = []\n",
    "tot_events = 0\n",
    "\n",
    "# old training files\n",
    "#testing_path =\"testing_samples/old_TestingSamples/DeepCoreTrainingSample*\"\n",
    "\n",
    "# new training files\n",
    "testing_path = \"/Users/liamoshaughnessy/Desktop/Research_2023_2024/Code/Python_Files/Semester_1/TestingSamples2/DeepCoreTraining*\"   \n",
    "#testing_path =\"testing_samples/TestingSamples2.2/DeepCoreTraining*\"\n",
    "#testing_path =\"testing_samples/TestingSamples2.2/DeepCoreTrainingSample_21*\"\n",
    "print(\"number of  testing files=\", len(glob.glob(testing_path)))\n",
    "\n",
    "for batch in Generator2(testing_path,count=True):\n",
    "        #pdb.set_trace()\n",
    "        tot_events += batch\n",
    "        \n",
    "jetNum = tot_events\n",
    "print(\"total number of events =\", jetNum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4f03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jetUps= (None, 30, 30, 2)\n",
      "NNinputs= (None, 30, 30, 4)\n",
      "ComplInput= (None, 30, 30, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 2)                    0         ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 1, 2)              0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 30, 30, 4)]          0         []                            \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 30, 30, 2)            0         ['reshape[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 30, 30, 6)            0         ['input_3[0][0]',             \n",
      " )                                                                   'up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 30, 30, 50)           14750     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 40)           50040     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 40)           40040     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 30)           30030     ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 30)           8130      ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 30, 30, 18)           4878      ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 30, 30, 30)           8130      ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 18)           2934      ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 30, 30, 30)           8130      ['conv2d_9[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 30, 30, 18)           2934      ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 30, 30, 30)           8130      ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 30, 30, 18)           2934      ['conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 30, 30, 6)            1626      ['conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 30, 30, 3, 6)         0         ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 30, 30, 3, 2)         0         ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 182686 (713.62 KB)\n",
      "Trainable params: 182686 (713.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DeepCore 2.2 Architecture (Older Architectures at the end)\n",
    "from keras.layers import AlphaDropout\n",
    "\n",
    "NNinputs_jeta = Input(shape=(1,))\n",
    "NNinputs_jpt = Input(shape=(1,))\n",
    "NNinputsJet = concatenate([NNinputs_jeta,NNinputs_jpt])\n",
    "jetReshaped = Reshape((1,1,2))(NNinputsJet)\n",
    "jetUps = UpSampling2D(size=(jetDim,jetDim), data_format=\"channels_last\")(jetReshaped)\n",
    "print(\"jetUps=\", jetUps.shape)\n",
    "NNinputs = Input(shape=(jetDim,jetDim,layNum))\n",
    "print(\"NNinputs=\", NNinputs.shape)\n",
    "ComplInput = concatenate([NNinputs,jetUps],axis=3)\n",
    "print(\"ComplInput=\", ComplInput.shape)\n",
    "\n",
    "   \n",
    "conv30_9 = Conv2D(50,7, data_format=\"channels_last\", input_shape=(jetDim,jetDim,layNum+2), activation='relu',padding=\"same\")(ComplInput)\n",
    "conv30_7 = Conv2D(40,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_9)\n",
    "conv30_5 = Conv2D(40,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_7)#\n",
    "conv20_5 = Conv2D(30,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_5)\n",
    "conv15_5 = Conv2D(30,3, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv20_5)\n",
    "\n",
    "conv15_3_1 = Conv2D(18,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_5)\n",
    "conv15_3_2 = Conv2D(18,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_3_1)\n",
    "conv15_3_3 = Conv2D(18,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_3_2) #(12,3)\n",
    "conv15_3 = Conv2D(18,3, data_format=\"channels_last\",padding=\"same\")(conv15_3_3) #(12,3)\n",
    "reshaped = Reshape((jetDim,jetDim,overlapNum,parNum+1))(conv15_3)\n",
    "\n",
    "conv12_3_1 = Conv2D(30,3, data_format=\"channels_last\", activation='relu', padding=\"same\")(conv15_5)  #new\n",
    "conv1_3_2 = Conv2D(30,3, data_format=\"channels_last\", activation='relu', padding=\"same\")(conv12_3_1) #drop7lb   #new\n",
    "conv1_3_3 = Conv2D(30,3, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv1_3_2) #new\n",
    "conv1_3_1 = Conv2D(6,3, data_format=\"channels_last\", activation='sigmoid', padding=\"same\")(conv1_3_3)\n",
    "reshaped_prob = Reshape((jetDim,jetDim,overlapNum,2))(conv1_3_1)\n",
    "\n",
    "model = Model([NNinputs,NNinputs_jeta,NNinputs_jpt],[reshaped,reshaped_prob])\n",
    "anubi = tf.keras.optimizers.Adam(learning_rate=0.0000001)#after epochs 252 (with septs/20 and batch_size 64)\n",
    "\n",
    "model.compile(optimizer=anubi, loss=[loss_mse_select_clipped,loss_ROIsoft_crossentropy], loss_weights=[1,1]) #FOR LATE TRAINING\n",
    "#model.compile(optimizer=anubi, loss=[loss_mse_select_clipped,loss_ROI_crossentropy], loss_weights=[1,1]) #FOR EARLY TRAINING\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42eddbd",
   "metadata": {},
   "source": [
    "# Small stats\n",
    "If you want to run on a small subset of the testing samples, you can run the cells below.\n",
    "\n",
    "If you want to use more stats, go directly to the More Stats section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de9000ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 98ms/step\n"
     ]
    }
   ],
   "source": [
    "# Loading DeepCore model and evaluation on small stats\n",
    "\n",
    "inputs = []\n",
    "chunk = next(Generator2(testing_path,batch,))\n",
    "inputs.append(chunk[0])\n",
    "\n",
    "## Importing model\n",
    "#model.load_weights('Valerio_model.hdf5') # DeepCore 1.0\n",
    "#model.load_weights('DeepCore_model_1017.h5') # DeepCore 2.0\n",
    "#model.load_weights('DeepCore_model_23_0214_40.h5') DeepCore 2.1\n",
    "model.load_weights('/Users/liamoshaughnessy/Desktop/Research_2023_2024/Code/Python_Files/Semester_1/DeepCore_model_0622.h5') # DeepCore 2.2\n",
    "#model.summary()\n",
    "\n",
    "## Making all the predictions\n",
    "[validation_par,validation_prob] = model.predict(inputs)\n",
    "validation_par = np.float64(validation_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0449c",
   "metadata": {},
   "source": [
    "The format of the prediction arrays are as follow: \n",
    "\n",
    "1) validation_prob[merged_cluster, x,  y, track, [probability, fake probability]]\n",
    "\n",
    "2) validation_par[merged_cluster, x,  y, track, [dx, dy, deta, dphi, 1/pt, zero_flag]] \n",
    "\n",
    "- Your inputs are merged cluster --> N\n",
    "\n",
    "- Your image size is x = 30 by y = 30.\n",
    "\n",
    "- For every pixel (30*30 = 900) in every merged cluster there are 3 predicted tracks so track = 3.\n",
    "\n",
    "- For every predicted track in every pixel of every merged cluster, you have a prediction score in validation_prob (how likely that a particle crossed this pixel) and predicted parameters in validation_par (dx, dy, deta, dphi, 1/pt). Please ignore \"fake probability\" and \"zero_flag\".\n",
    "\n",
    "If this is a bit confusing, I've included a few useful print outs below to convince yourself of this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a8192b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(357, 30, 30, 3, 2)\n",
      "(357, 30, 30, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "# Debugging printouts\n",
    "print(validation_prob.shape) # (merged cluster, x, y, track, prediction)\n",
    "print(validation_par.shape) # 5 predicted track param and predicted zero flag\n",
    "#print(validation_prob)\n",
    "#print(validation_prob[0]) # prints out everything for input 0 (1st merged hit)\n",
    "#print(validation_prob[0,0,0]) # prints out everything for input 0 (1st merged hit) and x = y = 0\n",
    "#print(validation_prob[0,0,0,0]) # prints out everything for input 0 (1st merged hit) and x = y = track = 0\n",
    "#print(validation_prob[0,0,0,0,0]) # prints prediction (ignore 2nd prediction since it's softmax) for input 0 and x = y = track = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52709a8a",
   "metadata": {},
   "source": [
    "The format of the target/truth information has similar formatting to the prediction, but it included in the Training sample input array, which also contains the CNN input information.\n",
    "\n",
    "For DeepCore to make a prediction, it requires this input information for every merged cluster: the charge deposit on every pixel of every layer (30*30*4), jet pt and jet eta.\n",
    "\n",
    "The target information is included in chunk[1], while the input information is included in chunk[0]. \n",
    "\n",
    "- Target Track Parameters: chunk[1][0][merged_cluster][x][y][track][dx,dy,deta,dphi,1/pt,prob_flag] (ignore \"prob_flag\").\n",
    "\n",
    "- Target Track Score: chunk[1][1][merged_cluster][x][y][track][target_score] (1 if a particle crossed a pixel, 0 otherwise).\n",
    "\n",
    "- Merged cluster charge deposit: chunk[0][0][merged_cluster][x][y][layer] (4 layers)\n",
    "\n",
    "- Merged cluster jet eta: chunk[0][1][merged_cluster]\n",
    "\n",
    "- Merged cluster jet pt: chunk[0][2][merged_cluster]\n",
    "\n",
    " I've included a few useful print outs below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2e37255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging printouts\n",
    "#print(chunk[1][0][0][0][0][0][0]) # prints target parameter dx for 1st merged cluster and x = y = track = 0\n",
    "#print(chunk[1][1][0][0][0][0][0]) # prints target score (0 or 1) for 1st merged cluster and x = y = track = 0\n",
    "#print(chunk[0][0][0][0][0][1]) # prints charge deposit in a pixel on layer 2 for 1st Merged cluster and x = y = 0 \n",
    "#print(chunk[0][1][1]) # print jet eta of 2nd merged cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7067d",
   "metadata": {},
   "source": [
    "A couple more print outs for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "779cba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged cluster, x, y,   pred score, true dx,  true dy, true deta, true dphi, true 1/pt\n",
      "     0          3 4    0.9287    -0.1142    0.5292    2.1652    2.1201    12.1672\n",
      "     0          11 18    0.8673    0.2542    0.6913    -0.5282    0.6381    43.3855\n",
      "     0          14 16    0.9692    0.3575    0.1619    -0.1688    0.123    105.0546\n",
      "     0          15 14    0.9731    0.3047    -0.1133    0.1857    0.0007    67.4644\n",
      "     0          15 14    0.9128    0.1577    0.4262    0.1276    -0.0391    693.5627\n",
      "     0          23 14    0.9097    -0.0199    0.0    0.2241    -1.3393    29.0169\n",
      "     0          23 14    0.842    -0.0199    0.0    0.2241    -1.3393    29.0169\n",
      "     1          22 14    0.901    -0.3767    0.0586    0.2372    -0.353    5.9771\n",
      "     2          14 14    0.9279    -0.2163    0.5551    0.2317    0.1152    141.1419\n",
      "     2          14 15    0.9429    -0.156    0.722    -0.0147    0.1397    487.2885\n",
      "     2          15 15    0.9755    0.2886    -0.4678    0.1583    -0.1021    279.4931\n",
      "     2          15 16    0.9814    0.3816    0.3931    -0.19    -0.0861    209.1883\n",
      "     3          16 12    0.9292    -0.2163    0.5551    0.5554    -0.1831    141.1419\n",
      "     3          16 13    0.9422    -0.156    0.722    0.309    -0.1586    487.2885\n",
      "     3          17 13    0.979    0.2886    -0.4678    0.4821    -0.4005    279.4931\n",
      "     3          17 14    0.9861    0.3816    0.3931    0.1338    -0.3844    209.1883\n",
      "     4          13 13    0.8948    0.3049    0.565    0.398    0.4686    85.1442\n",
      "     4          13 17    0.9718    -0.1016    -0.4862    -0.2945    0.3208    23.4523\n",
      "     4          14 12    0.9796    -0.3703    0.1847    0.6611    0.2099    278.0162\n",
      "     4          14 12    0.927    0.9844    0.2155    0.6555    -0.1066    29.6505\n",
      "     4          14 20    0.9744    0.5035    0.3896    -1.0509    -0.2683    10.3935\n",
      "     4          18 9    0.9767    -0.1234    0.345    1.2682    -0.2404    14.9314\n",
      "     4          28 6    0.8072    -0.4046    0.5661    1.8845    -1.7419    17.135\n",
      "     5          20 19    0.9099    0.3049    0.565    -0.9948    -0.8634    85.1442\n",
      "     5          20 23    0.9717    -0.1016    -0.4862    -1.6873    -1.0112    23.4523\n",
      "     5          21 17    0.921    -0.902    -0.4922    -0.426    -1.0218    583.2363\n",
      "     5          21 18    0.983    -0.3703    0.1847    -0.7316    -1.1221    278.0162\n",
      "     5          21 18    0.944    0.9844    0.2155    -0.7373    -1.4386    29.6505\n",
      "     5          21 26    0.9756    0.5035    0.3896    -2.4437    -1.6003    10.3935\n",
      "     5          25 15    0.9717    -0.1234    0.345    -0.1245    -1.5724    14.9314\n",
      "     6          13 11    0.9772    -0.4527    0.1368    0.9989    0.2519    50.3636\n",
      "     6          13 13    0.9857    0.1022    0.3315    0.517    0.2389    1001.1696\n",
      "     6          13 13    0.9775    0.3221    0.293    0.5229    0.2162    540.2963\n",
      "     7          11 12    0.9073    -0.254    -0.6188    0.7689    0.7215    279.716\n",
      "     7          11 13    0.9668    0.0149    -0.3923    0.5165    0.6725    790.9153\n",
      "     7          12 16    0.969    0.3663    0.4626    -0.2678    0.3636    38.487\n",
      "     7          12 19    0.9781    0.3593    -0.4012    -0.8001    0.5907    29.4438\n",
      "     7          13 13    0.9654    0.4992    -0.1749    0.491    0.2611    105.9299\n",
      "     8          19 27    0.9405    0.1167    -0.7061    -2.5429    -0.4131    16.9043\n",
      "     8          20 2    0.8181    -0.254    -0.6188    2.9214    -0.7182    279.716\n",
      "     8          20 3    0.9004    0.0149    -0.3923    2.6689    -0.7671    790.9153\n",
      "     8          21 6    0.971    0.3663    0.4626    1.8847    -1.076    38.487\n",
      "     8          21 9    0.9745    0.3593    -0.4012    1.3524    -0.8489    29.4438\n",
      "     8          22 3    0.9633    0.4992    -0.1749    2.6435    -1.1785    105.9299\n",
      "     8          24 22    0.892    -0.4476    -0.7039    -1.4393    -1.428    30.3239\n",
      "     9          8 15    0.8459    -0.4881    -0.165    -0.0324    0.969    26.555\n",
      "     9          14 15    0.9798    -0.2576    -0.4204    -0.0059    0.1579    1089.6777\n",
      "     9          14 15    0.8626    -0.2225    -0.7711    0.0463    0.1424    512.5205\n",
      "     9          15 11    0.9762    -0.2575    -0.6382    1.2566    0.1519    25.05\n",
      "     9          15 15    0.988    0.2622    0.0449    -0.0797    -0.1226    81.4687\n"
     ]
    }
   ],
   "source": [
    "# Debugging printouts: Printing prediction, target probility and track number if target = 1\n",
    "N = 10\n",
    "print(\"merged cluster, x, y,   pred score, true dx,  true dy, true deta, true dphi, true 1/pt\")\n",
    "for i in range(N): # loop over input\n",
    "    for j in range(30): # loop over x\n",
    "        for k in range(30): # loop over y\n",
    "            for l in range(3) : # loop over tracks\n",
    "                if chunk[1][1][i][j][k][l][0] ==1 and validation_prob[i,j,k,l,0]>0.8:\n",
    "                    print(\"    \",i,\"        \", j, k, \"  \", round(validation_prob[i,j,k,l,0],4),\"  \", round(chunk[1][0][i][j][k][l][0],4),\"  \", round(chunk[1][0][i][j][k][l][1], 4),\"  \", round(chunk[1][0][i][j][k][l][2], 4) ,\"  \", round(chunk[1][0][i][j][k][l][3],4) ,\"  \", round(chunk[1][0][i][j][k][l][4],4) )\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7e47733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel ( 3 , 4 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.9286535\n",
      "              dx:  -0.114  vs  -0.293\n",
      "              dy:  0.529  vs  0.127\n",
      "            deta:  2.165  vs  1.858\n",
      "            dphi:  2.12  vs  1.846\n",
      "            1/pt:  12.167  vs  19.215\n",
      "1747.949951171875\n",
      "Pixel ( 11 , 18 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.86727995\n",
      "              dx:  0.254  vs  0.038\n",
      "              dy:  0.691  vs  0.781\n",
      "            deta:  -0.528  vs  -0.629\n",
      "            dphi:  0.638  vs  0.569\n",
      "            1/pt:  43.386  vs  60.675\n",
      "1747.949951171875\n",
      "Pixel ( 14 , 16 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.9692073\n",
      "              dx:  0.358  vs  0.212\n",
      "              dy:  0.162  vs  0.032\n",
      "            deta:  -0.169  vs  0.054\n",
      "            dphi:  0.123  vs  -0.017\n",
      "            1/pt:  105.055  vs  167.343\n",
      "1747.949951171875\n",
      "Pixel ( 15 , 14 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.9730973\n",
      "              dx:  0.305  vs  0.143\n",
      "              dy:  -0.113  vs  0.246\n",
      "            deta:  0.186  vs  0.072\n",
      "            dphi:  0.001  vs  -0.096\n",
      "            1/pt:  67.464  vs  194.048\n",
      "1747.949951171875\n",
      "Pixel ( 19 , 9 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.685007\n",
      "              dx:  -0.449  vs  -0.942\n",
      "              dy:  -0.219  vs  -0.192\n",
      "            deta:  1.004  vs  1.127\n",
      "            dphi:  -0.61  vs  -0.432\n",
      "            1/pt:  53.231  vs  32.28\n",
      "1747.949951171875\n",
      "Pixel ( 22 , 16 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.72212505\n",
      "              dx:  0.485  vs  0.143\n",
      "              dy:  0.308  vs  0.978\n",
      "            deta:  -0.358  vs  -0.127\n",
      "            dphi:  -1.019  vs  -0.99\n",
      "            1/pt:  36.023  vs  37.291\n",
      "1747.949951171875\n",
      "Pixel ( 23 , 14 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.90965235\n",
      "              dx:  -0.02  vs  -0.16\n",
      "              dy:  0.0  vs  -0.649\n",
      "            deta:  0.224  vs  0.236\n",
      "            dphi:  -1.339  vs  -0.974\n",
      "            1/pt:  29.017  vs  72.358\n",
      "1747.949951171875\n",
      "Pixel ( 22 , 14 ): True     vs   Predicted\n",
      "     Probability: 1.0      vs  0.90096277\n",
      "              dx:  -0.377  vs  -0.827\n",
      "              dy:  0.059  vs  -0.005\n",
      "            deta:  0.237  vs  0.125\n",
      "            dphi:  -0.353  vs  -0.896\n",
      "            1/pt:  5.977  vs  16.99\n",
      "1747.949951171875\n"
     ]
    }
   ],
   "source": [
    "# Debugging printouts: Printing prediction, target probility and track number if target = 1\n",
    "N = 2\n",
    "for i in range(N): # loop over input\n",
    "    for j in range(30): # loop over x\n",
    "        for k in range(30): # loop over y\n",
    "            for l in range(1) : # loop over tracks\n",
    "                # TCP\n",
    "                pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                # True TP\n",
    "                pixel_true_dx = chunk[1][0][i][j][k][l][0]\n",
    "                pixel_true_dy = chunk[1][0][i][j][k][l][1]\n",
    "                pixel_true_deta = chunk[1][0][i][j][k][l][2]\n",
    "                pixel_true_dphi = chunk[1][0][i][j][k][l][3]\n",
    "                pixel_true_invpt = chunk[1][0][i][j][k][l][4]\n",
    "                # Predicted TP\n",
    "                pixel_pred_dx = validation_par[i,j,k,l,0]\n",
    "                pixel_pred_dy = validation_par[i,j,k,l,1]\n",
    "                pixel_pred_deta = validation_par[i,j,k,l,2]\n",
    "                pixel_pred_dphi = validation_par[i,j,k,l,3]\n",
    "                pixel_pred_invpt = validation_par[i,j,k,l,4]\n",
    "                # merged cluster inputs (layer 2)\n",
    "                pixel_lay2_adc = chunk[0][0][i][j][k][1]\n",
    "                merged_cluster_eta = chunk[0][1][i]\n",
    "                merged_cluster_pt = chunk[0][2][i]\n",
    "                if pixel_true_prob ==1: \n",
    "                    print(\"Pixel (\",j,\",\",k,\"): True     vs   Predicted\")\n",
    "                    print(\"     Probability:\",  round(pixel_true_prob,3), \"     vs \", pixel_pred_prob )\n",
    "                    print(\"              dx: \",  round(pixel_true_dx,3), \" vs \",  round(pixel_pred_dx, 3) )\n",
    "                    print(\"              dy: \",  round(pixel_true_dy,3), \" vs \",  round(pixel_pred_dy, 3) )\n",
    "                    print(\"            deta: \",  round(pixel_true_deta,3), \" vs \",  round(pixel_pred_deta, 3) )\n",
    "                    print(\"            dphi: \",  round(pixel_true_dphi,3), \" vs \",  round(pixel_pred_dphi, 3) )\n",
    "                    print(\"            1/pt: \",  round(pixel_true_invpt,3), \" vs \",  round(pixel_pred_invpt, 3) )\n",
    "                    print(merged_cluster_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c45ac",
   "metadata": {},
   "source": [
    "# Score Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3913bd",
   "metadata": {},
   "source": [
    "The Score plots are important as they allow us to pick an appropriate threshold for a given training. You should always make these plots and use them to select ta threshold before evalauting your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "968f5e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 357 is out of bounds for axis 0 with size 357",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#l = 2\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m---> 24\u001b[0m     A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     25\u001b[0m     S\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(A, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     26\u001b[0m     SS \u001b[38;5;241m=\u001b[39m S[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 357 is out of bounds for axis 0 with size 357"
     ]
    }
   ],
   "source": [
    "# Signal is true tracks with predictions, background is when there is a prediction for a track that does not exist (fake)\n",
    "signal = []\n",
    "signal_target = []\n",
    "background = []\n",
    "\n",
    "signal_lay2 = []\n",
    "background_lay2 = []\n",
    "\n",
    "signal_lay2_0 = []\n",
    "background_lay2_0 = []\n",
    "\n",
    "truth = []\n",
    "pred = []\n",
    "\n",
    "N = 3700 # number of inputs\n",
    "Nx = 30 # X dimension\n",
    "Ny = 30 # Y dimension\n",
    "Ntrk = 3 # Number of tracks\n",
    "ble = 0\n",
    "tot = 0\n",
    "#l = 2\n",
    "\n",
    "for i in range(N):\n",
    "    A = np.array(chunk[0][0][i])\n",
    "    S= np.sum(A, axis=(0,1))\n",
    "    SS = S[1]\n",
    "    #merged_cluster_eta = chunk[0][1][i]\n",
    "    #if merged_cluster_eta > 0.4:\n",
    "    #    ble = ble + 1\n",
    "    #    continue\n",
    "    #SS = S[0]+S[1]+S[2]+S[3]\n",
    "    if SS > 0 :\n",
    "    #    ble = ble + 1\n",
    "        continue\n",
    "    for j in range(Nx):\n",
    "        for k in range(Ny):\n",
    "            for l in range(Ntrk) :\n",
    "                l = 0    #to look at Overlap maps, just set l = 0/1/2 here\n",
    "                pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                pixel_lay2_adc = chunk[0][0][i][j][k][1]\n",
    "                truth.append(pixel_true_prob)\n",
    "                pred.append(pixel_pred_prob)\n",
    "                #pred.append(validation_prob[i,j,k,l,0]/(validation_prob[i,j,k,l,0]+validation_prob[i,j,k,l,1]))\n",
    "                #if  pixel_lay2_adc ==0:\n",
    "                #    continue\n",
    "                if pixel_true_prob ==1 :\n",
    "                    signal.append(pixel_pred_prob)\n",
    "                    #if SS > 0:\n",
    "                    #    signal_lay2.append(validation_prob[i,j,k,l,0])\n",
    "                    #else:\n",
    "                    #    signal_lay2_0.append(validation_prob[i,j,k,l,0])\n",
    "                    #signal_target.append(chunk[1][0][i][j][k][l][:5])\n",
    "                    #signal.append(validation_prob[i,j,k,l,0]/(validation_prob[i,j,k,l,0]+validation_prob[i,j,k,l,1]))\n",
    "                    \n",
    "                else:\n",
    "                    background.append(pixel_pred_prob)\n",
    "                    #if SS > 0:\n",
    "                    #    background_lay2.append(validation_prob[i,j,k,l,0])\n",
    "                    #else:\n",
    "                    #    background_lay2_0.append(validation_prob[i,j,k,l,0])\n",
    "                    #background.append(validation_prob[i,j,k,l,0]/(validation_prob[i,j,k,l,0]+validation_prob[i,j,k,l,1]))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import metrics\n",
    "\n",
    "x = [0,1]\n",
    "\n",
    "fpr_keras, tpr_keras, _ = metrics.roc_curve(truth, pred)\n",
    "auc = np.trapz(tpr_keras,fpr_keras)\n",
    "#print auc\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc))\n",
    "plt.plot(x, x, color='red')  \n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve (area = {:.3f})'.format(auc))\n",
    "plt.savefig(\"ROC.png\")\n",
    "plt.savefig(\"ROC.pdf\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Prediction Score Distribution (log, not normalized): recommended\n",
    "bin_size = 1000\n",
    "#bin_size = 50\n",
    "plt.hist(signal, alpha = 0.5, color = 'b', label = 'Signal', range = (0,1), bins = bin_size) #, density = True)\n",
    "plt.hist(background, color = 'r', alpha = 0.5, label = 'Background', range = (0,1), bins = bin_size) #, density = True)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Prediction Score')\n",
    "plt.ylabel('Predicted TCPs')\n",
    "plt.yscale('log')\n",
    "plt.title('Prediction Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Prediction Score Distribution (normalized): misleading but useful to compare discrimination power of models\n",
    "bin_size = 1000\n",
    "#bin_size = 50\n",
    "plt.hist(signal, alpha = 0.5, color = 'b', label = 'Signal', range = (0,1), bins = bin_size, density = True)\n",
    "plt.hist(background, color = 'r', alpha = 0.5, label = 'Background', range = (0,1), bins = bin_size, density = True)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Prediction Score')\n",
    "plt.ylabel('Predicted TCPs')\n",
    "plt.yscale('log')\n",
    "plt.title('Prediction Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5058e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score plots with adc = or > 0 cut given layer and overlap: Useful to look at score plots as a function of Overlap and zero-adc layers\n",
    "def score_plot_lay_pos(lay, overlap):\n",
    "    signal = []\n",
    "    background = []\n",
    "    N = 3800 # number of inputs\n",
    "    Nx = 30 # X dimension\n",
    "    Ny = 30 # Y dimension\n",
    "    l = overlap\n",
    "    frac = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        A = np.array(chunk[0][0][i])\n",
    "        S= np.sum(A, axis=(0,1))\n",
    "        SS = S[lay]\n",
    "        if SS == 0 :\n",
    "            continue\n",
    "        frac = frac + 1\n",
    "        for j in range(Nx):\n",
    "            for k in range(Ny):\n",
    "                pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                if pixel_true_prob ==1:\n",
    "                    signal.append(pixel_pred_prob)\n",
    "                else:\n",
    "                    background.append(pixel_pred_prob)\n",
    "   \n",
    "    bin_size = 1000\n",
    "    plt.hist(signal, alpha = 0.5, color = 'b', label = 'Signal', range = (0,1), bins = bin_size) #, density = True)\n",
    "    plt.hist(background, color = 'r', alpha = 0.5, label = 'Background', range = (0,1), bins = bin_size) #, density = True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Prediction Score')\n",
    "    plt.ylabel('Pixels')\n",
    "    plt.yscale('log')\n",
    "    s = 'Distribution of Overlap ' + str(l+1) + ' with $\\Sigma$adc(layer '+ str(lay+1) + ') > 0 (f=' + str(round((100*frac)/N,1))+ '%)'\n",
    "    plt.title(s)\n",
    "    plt.show()\n",
    "    \n",
    "def score_plot_lay_zero(lay, overlap):\n",
    "    signal = []\n",
    "    background = []\n",
    "    N = 3800 # number of inputs\n",
    "    Nx = 30 # X dimension\n",
    "    Ny = 30 # Y dimension\n",
    "    l = overlap\n",
    "    frac = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        A = np.array(chunk[0][0][i])\n",
    "        S= np.sum(A, axis=(0,1))\n",
    "        SS = S[lay]\n",
    "        if SS > 0 :\n",
    "            continue\n",
    "        frac = frac + 1\n",
    "        for j in range(Nx):\n",
    "            for k in range(Ny):\n",
    "                pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                if pixel_true_prob ==1:\n",
    "                    signal.append(pixel_pred_prob)\n",
    "                else:\n",
    "                    background.append(pixel_pred_prob)\n",
    "   \n",
    "    bin_size = 1000\n",
    "    plt.hist(signal, alpha = 0.5, color = 'b', label = 'Signal', range = (0,1), bins = bin_size) #, density = True)\n",
    "    plt.hist(background, color = 'r', alpha = 0.5, label = 'Background', range = (0,1), bins = bin_size) #, density = True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Prediction Score')\n",
    "    plt.ylabel('Pixels')\n",
    "    plt.yscale('log')\n",
    "    s = 'Distribution of Overlap ' + str(l+1) + ' with $\\Sigma$adc(layer '+ str(lay+1) + ') = 0 (f=' + str(round((100*frac)/N,1))+ '%)'\n",
    "    plt.title(s)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lay in range(0,4):\n",
    "    for overlap in range(0,3):\n",
    "        score_plot_lay_pos(lay, overlap)\n",
    "        score_plot_lay_zero(lay, overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b36b8",
   "metadata": {},
   "source": [
    "# Parameter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42e98e86",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 357 is out of bounds for axis 0 with size 357",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 219\u001b[0m\n\u001b[1;32m    217\u001b[0m dth_l134 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    218\u001b[0m dth_l2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 219\u001b[0m A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    220\u001b[0m S\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(A, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (S[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mS[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39mS[\u001b[38;5;241m3\u001b[39m]):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 357 is out of bounds for axis 0 with size 357"
     ]
    }
   ],
   "source": [
    "# DeepCore 2.2.1 Param plots definition and set up\n",
    "\n",
    "# Residual Plots\n",
    "def residual_plots(par, bins):\n",
    "    if(par== 0) :\n",
    "        pylab.hist(bins,80,facecolor='darkorange',alpha=0.75,range=(-150,150))\n",
    "        pylab.title('Residual Distribution - $\\Delta x$',fontsize=22)\n",
    "        # Plotting the mean and sd if needed\n",
    "        #mean = np.array(bins).mean()\n",
    "        #sigma = np.array(bins).std()\n",
    "        #plt.text(80, 50000, \"Mean =%.2f $\\mu$m\"%(mean), size=14) \n",
    "        #plt.text(80, 60000, \"$\\sigma_{\\Delta x}$ = %.1f $\\mu$m\"%(sigma), size=14)\n",
    "        # the tricky part is adjusting the height (50000 for the mean) every time you change statistics\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        pylab.xlabel('Prediction-Target [$\\mu$m]',fontsize=22)\n",
    "        plt.grid(True)\n",
    "    if(par == 1) :\n",
    "        pylab.hist(bins,120, facecolor='darkorange',alpha=0.75,range=(-200,200))\n",
    "        pylab.title('Residual Distribution - $\\Delta y$',fontsize=22)\n",
    "        #mean = np.array(bins).mean()\n",
    "        #sigma = np.array(bins).std()\n",
    "        #plt.text(100, 50000, \"Mean =%.1f $\\mu$m\"%(mean), size=14)\n",
    "        #plt.text(100, 60000, \"$\\sigma_{\\Delta y}$ = %.1f $\\mu$m\"%(sigma), size=14)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        pylab.xlabel('Prediction-Target [$\\mu$m]',fontsize=22)\n",
    "        plt.grid(True)\n",
    "    if(par== 2):\n",
    "        pylab.hist(bins,100, facecolor='darkorange',alpha=0.75,range=(-0.02,0.02))\n",
    "        pylab.title('Residual Distribution - $\\Delta \\eta$',fontsize=22)\n",
    "        #mean = np.array(bins).mean()\n",
    "        #sigma = np.array(bins).std()\n",
    "        #plt.text(0.01, 130000, \"Mean = 0\", size=14)\n",
    "        #plt.text(0.01, 150000, \"$\\sigma_{\\Delta \\eta}$ = %.3f\"%(sigma), size=14)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        pylab.xlabel('Prediction-Target',fontsize=22)\n",
    "        plt.grid(True)\n",
    "    if(par == 3):\n",
    "        pylab.hist(bins,100, facecolor='darkorange',alpha=0.75,range=(-0.02,0.02))\n",
    "        pylab.title('Residual Distribution - $\\Delta \\phi$',fontsize=22)\n",
    "        #mean = np.array(bins).mean()\n",
    "        #sigma = np.array(bins).std()\n",
    "        #plt.text(0.01, 80000, \"Mean = 0\", size=14)\n",
    "        #plt.text(0.01, 90000, \"$\\sigma_{\\Delta \\phi}$ = %.3f\"%(sigma), size=14)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        pylab.xlabel('Prediction-Target',fontsize=22)\n",
    "        plt.grid(True)\n",
    "    if (par == 4) :\n",
    "        pylab.hist(bins,150, facecolor='darkorange', alpha=0.75,range=(-3,12))#range=(-0.2,0.2))#range=(-2,5)) #relative\n",
    "        pylab.title('Residual Distribution - p_T$',fontsize=22)\n",
    "        #mean = np.array(bins).mean()\n",
    "        #sigma = np.array(bins).std()\n",
    "        #plt.text(4, 28000, \"Mean =%.1f\"%(mean), size=14)\n",
    "        #plt.text(4, 25000, \"$\\sigma_{\\Delta(1/p_T)rel}$ = %.1f\"%(sigma), size=14)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        pylab.xlabel('(Prediction-Target)/Target',fontsize=18) #only 1/pt\n",
    "        plt.grid(True)\n",
    "    plt.show()     \n",
    "\n",
    "# Target Plots\n",
    "def target_plots(par, bins):\n",
    "    if(par == 0):\n",
    "        pylab.hist(bins,100, facecolor='royalblue', alpha=0.75,range=(-100,100))\n",
    "        pylab.title('Target Distribution - $\\Delta x$',fontsize=22)\n",
    "        pylab.xlabel('Target [$\\mu$m]',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if(par == 1) :\n",
    "        pylab.hist(bins,100, facecolor='royalblue', alpha=0.75,range=(-150,150))\n",
    "        pylab.title('Target Distribution - $\\Delta y$',fontsize=22)\n",
    "        pylab.xlabel('Target [$\\mu$m]',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if(par == 2) :\n",
    "        pylab.hist(bins,90, facecolor='royalblue', alpha=0.75,range=(-0.04,0.04))\n",
    "        pylab.title('Target Distribution - $\\Delta \\eta$',fontsize=22)\n",
    "        pylab.xlabel('Target',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if(par == 3) :\n",
    "        pylab.hist(bins,90, facecolor='royalblue', alpha=0.75,range=(-0.04,0.04))\n",
    "        pylab.title('Target Distribution - $\\Delta \\phi$',fontsize=22)\n",
    "        pylab.xlabel('Target',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if (par ==4):\n",
    "        pylab.hist(bins,100, facecolor='royalblue',alpha=0.75,range=(0,200))\n",
    "        pylab.title('Target Distribution - $p_T$',fontsize=22)\n",
    "        pylab.xlabel('Target [GeV]',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Prediction Plots\n",
    "def pred_plots(par, bins):\n",
    "    if(par == 0):\n",
    "        pylab.hist(bins,80, facecolor='red', alpha=0.75,range=(-150,150))\n",
    "        pylab.title('Prediction Distribution - $\\Delta x$',fontsize=22)\n",
    "        pylab.xlabel('Prediction [$\\mu$m]',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if(par ==1) :\n",
    "        pylab.hist(bins,120, facecolor='red', alpha=0.75,range=(-150,150))\n",
    "        pylab.title('Prediction Distribution - $\\Delta y$',fontsize=22)\n",
    "        pylab.xlabel('Prediction [$\\mu$m]',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if(par == 2) :\n",
    "        pylab.hist(bins,70, facecolor='red', alpha=0.75,range=(-0.03,0.03))\n",
    "        pylab.title('Prediction Distribution - $\\Delta \\eta$',fontsize=22)\n",
    "        pylab.xlabel('Prediction',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if(par ==3 ) :\n",
    "        pylab.hist(bins,70, facecolor='red', alpha=0.75,range=(-0.03,0.03))\n",
    "        pylab.title('Prediction Distribution - $\\Delta \\phi$',fontsize=22)\n",
    "        pylab.xlabel('Prediction',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    if (par ==4) :\n",
    "        pylab.hist(bins,100, facecolor='red',alpha=0.75,range=(0,200))\n",
    "        pylab.title('Prediction Distribution - $p_T$',fontsize=22)\n",
    "        pylab.xlabel('Prediction [GeV]',fontsize=18)\n",
    "        pylab.ylabel('TCP',fontsize=18)\n",
    "        plt.grid(True)\n",
    "    plt.show()\n",
    "        \n",
    "# Scatter Plots\n",
    "def scatter_plots(par, bins_x, bins_y):\n",
    "    if(par == 0) :\n",
    "        nx = 100\n",
    "        plt.hist2d(bins_x, bins_y,bins=nx,range= [[-nx,nx], [-nx, nx]], cmap=plt.cm.viridis)#, marker='+')\n",
    "        # Plotting diagonal\n",
    "        l_x = [-150, 150]\n",
    "        plt.plot(l_x, l_x, color='black') \n",
    "        pylab.title('Prediction vs Target - $\\Delta x$',fontsize=22)\n",
    "        plt.ylabel('Predicted $\\Delta x$ [$\\mu$m]', fontsize=18)\n",
    "        plt.xlabel('Target $\\Delta x$ [$\\mu$m]', fontsize=18)\n",
    "        plt.colorbar()\n",
    "        plt.grid(False)\n",
    "    if(par == 1) :\n",
    "        ny = 150\n",
    "        plt.hist2d(bins_x,bins_y,bins=ny,range= [[-ny,ny], [-ny, ny]], cmap=plt.cm.viridis)\n",
    "        # Plotting diagonal\n",
    "        l_y = [-225,225]\n",
    "        plt.plot(l_y, l_y, color='black') \n",
    "        pylab.title('Prediction vs Target - $\\Delta y$',fontsize=22)\n",
    "        plt.ylabel('Predicted $\\Delta y$ [$\\mu$m]', fontsize=18)\n",
    "        plt.xlabel('Target $\\Delta y$ [$\\mu$m]', fontsize=18)\n",
    "        # Plotting other lines\n",
    "        #l_pt = [-75,75]\n",
    "        #l_zero = [0,0]\n",
    "        #l_1 = [10,10]\n",
    "        #l_2 = [-10,-10]\n",
    "        #plt.plot(l_pt, l_pt, color='black') \n",
    "        #plt.plot(l_pt, l_1, color='black', linestyle='dashed') \n",
    "        #plt.plot(l_pt, l_2, color='black', linestyle='dashed')\n",
    "        #plt.plot(l_pt, l_zero, color='black', linestyle='dashed') \n",
    "        plt.colorbar()\n",
    "        plt.grid(False)\n",
    "    if(par == 2) :\n",
    "        plt.hist2d(bins_x,bins_y,bins=180,range = [[-0.03,0.03], [-0.03, 0.03]], cmap=plt.cm.viridis)\n",
    "        # Plotting diagonal\n",
    "        l_eta = [-0.04,0.04]\n",
    "        plt.plot(l_eta, l_eta, color='black') \n",
    "        pylab.title('Prediction vs Target - $\\Delta \\eta$',fontsize=22)\n",
    "        plt.ylabel('Predicted $\\Delta \\eta$ ', fontsize=18)\n",
    "        plt.xlabel('Target $\\Delta \\eta$', fontsize=18)\n",
    "        plt.colorbar()\n",
    "        plt.grid(False)\n",
    "    if(par == 3) :\n",
    "        plt.hist2d(bins_x,bins_y,bins=180,range = [[-0.03,0.03], [-0.03, 0.03]], cmap=plt.cm.viridis)\n",
    "        # Plotting diagonal\n",
    "        l_phi = [-0.04,0.04]\n",
    "        plt.plot(l_phi, l_phi, color='black') \n",
    "        pylab.title('Prediction vs Target - $\\Delta \\phi$',fontsize=22)\n",
    "        plt.ylabel('Predicted $\\Delta \\phi$', fontsize=18)\n",
    "        plt.xlabel('Target $\\Delta \\phi$', fontsize=18)\n",
    "        plt.colorbar()\n",
    "        plt.grid(False)\n",
    "    if(par == 4) :\n",
    "        plt.hist2d(bins_x,bins_y,bins=100,range= [[0,200], [0, 200]])\n",
    "        # Plotting diagonal\n",
    "        l_pt = [0, 200]\n",
    "        #l_zero = [0,0]\n",
    "        plt.plot(l_pt, l_pt, color='black') \n",
    "        #plt.plot(l_zero, l_pt, color='black', linestyle='dashed') \n",
    "        #plt.plot(l_pt, l_zero, color='black', linestyle='dashed') \n",
    "        pylab.title('Prediction vs Target - $p_T$',fontsize=22)\n",
    "        plt.ylabel('Predicted $p_T$ [GeV]', fontsize=18)\n",
    "        plt.xlabel('Target $p_T$ [GeV]', fontsize=18)\n",
    "        plt.colorbar()\n",
    "        plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "parNum = 5 # TP\n",
    "N = 3700 # Merged clusters\n",
    "Nx = 30 # x dimension\n",
    "Ny = 30 # y dimension\n",
    "Nt = 3 # max Overlap\n",
    "\n",
    "#DeepCore 2.1.3 Thresholds\n",
    "prob_threshold = [0.7, 0.85, 1] \n",
    "prob_dth_l134 = [0.1, 0.05, 0]\n",
    "prob_dth_l2 = [0.3, 0.15, 0]\n",
    "\n",
    "jet_pt = []\n",
    "jet_eta = []\n",
    "pbins = [None] * parNum\n",
    "pbins_pred = [None] * parNum\n",
    "pbins_target = [None] * parNum\n",
    "for par in range(parNum) :\n",
    "    bins = []\n",
    "    bins_pred = []\n",
    "    bins_target = []\n",
    "    for i in range(N): # loop over input\n",
    "        dth_l134 = 1\n",
    "        dth_l2 = 1\n",
    "        A = np.array(chunk[0][0][i])\n",
    "        S= np.sum(A, axis=(0,1))\n",
    "        if (S[0]*S[2]*S[3]):\n",
    "            dth_l134 = 0\n",
    "        if (S[1]):\n",
    "            dth_l2 = 0\n",
    "        jet_pt.append(chunk[0][2][i])\n",
    "        jet_eta.append(chunk[0][1][i])\n",
    "        for j in range(Nx): # loop over x\n",
    "            for k in range(Ny): # loop over y\n",
    "                for l in range(Nt) : # loop over tracks\n",
    "                    pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                    pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                    pixel_lay2_adc = chunk[0][0][i][j][k][1]\n",
    "                    if (pixel_pred_prob > (prob_threshold[l] + dth_l134*prob_dth_l134[l] + dth_l2*prob_dth_l2[l] )) and (pixel_true_prob == 1) and (pixel_lay2_adc ==0) : \n",
    "                    #if (pixel_true_prob == 1) and (pixel_lay2_adc>0) : \n",
    "                        #threshold DeepCore 2.1.3\n",
    "                        if((par==0) or (par ==1)) : # dx or dy\n",
    "                            bins.append((validation_par[i,j,k,l,par] - chunk[1][0][i][j][k][l][par])*100)\n",
    "                            bins_pred.append(validation_par[i,j,k,l,par]*100)\n",
    "                            bins_target.append(chunk[1][0][i][j][k][l][par]*100)\n",
    "                        elif((par==2) or (par ==3)) : # deta or dphi\n",
    "                            bins.append((validation_par[i,j,k,l,par] - chunk[1][0][i][j][k][l][par])*0.01)\n",
    "                            bins_pred.append(validation_par[i,j,k,l,par]*0.01)\n",
    "                            bins_target.append(chunk[1][0][i][j][k][l][par]*0.01)\n",
    "                        else : #pt\n",
    "                            bins.append((validation_par[i,j,k,l,par] - chunk[1][0][i][j][k][l][par])/ chunk[1][0][i][j][k][l][par])  #relative\n",
    "                            bins_pred.append(validation_par[i,j,k,l,par])\n",
    "                            bins_target.append(chunk[1][0][i][j][k][l][par])\n",
    "    pbins[par] = bins\n",
    "    pbins_pred[par] = bins_pred\n",
    "    pbins_target[par] = bins_target\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc18cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0,5):\n",
    "    scatter_plots(n, pbins_target[n], pbins_pred[n])\n",
    "    #target_plots(n, pbins_target[n])\n",
    "    #pred_plots(n, pbins_pred[n])\n",
    "    plt.figure()\n",
    "#pred_plots(3, pbins_pred[3])\n",
    "#plt.figure()\n",
    "#target_plots(1, pbins_target[1])\n",
    "#plt.figure()\n",
    "#residual_plots(4, pbins[4])\n",
    "#plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet pt distribution\n",
    "bins_pt = 100\n",
    "plt.hist(jet_pt, bins=bins_pt, color='r', range = (500, 2500), alpha=0.5)\n",
    "plt.title('Jet $p_T$ Distrubution')\n",
    "plt.xlabel('$p_T^{jet}$')\n",
    "plt.ylabel('Merged Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6836eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet eta distribution\n",
    "bins_eta = 100\n",
    "plt.hist(jet_eta, bins=bins_eta, color='b', range = (-1.4, 1.4), alpha=0.5)\n",
    "plt.title('Jet $\\eta$ Distrubution')\n",
    "plt.xlabel('$\\eta^{jet}$')\n",
    "plt.ylabel('Merged Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a597726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet pt and eta plotted together (I prefer separately)\n",
    "bins_eta = 100\n",
    "bins_pt = 100\n",
    "fig, (j_eta, j_pt) = plt.subplots(2, 1)\n",
    "\n",
    "j_eta.hist(jet_eta, bins=bins_eta, color='b',range = (-1.4,1.4), alpha=0.5)\n",
    "j_eta.set_title('Jet $\\eta$ Distrubution')\n",
    "j_eta.set_xlabel('$\\eta^{jet}$')\n",
    "j_eta.set_ylabel('Merged Clusters')\n",
    "j_pt.hist(jet_pt, bins=bins_pt, color='r', range = (500, 2500), alpha=0.5)\n",
    "j_pt.set_title('Jet $p_T$ Distrubution')\n",
    "j_pt.set_xlabel('$p_T^{jet}$')\n",
    "j_pt.set_ylabel('Merged Clusters')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Jet_eta_and_pt.png\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010aca59",
   "metadata": {},
   "source": [
    "# TCP maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear propagation to the 4 barrel layers, with plotting purpose only\n",
    "def prop_on_layer(x1,y1,eta,phi,eta_jet,lay) :\n",
    "\n",
    "    theta_jet = 2*math.atan(math.exp(-eta_jet))\n",
    "    eta = eta+eta_jet\n",
    "    theta = 2*math.atan(math.exp(-eta))\n",
    "\n",
    "    if(lay==0) :\n",
    "        dist=3-6.8\n",
    "    if(lay==1) :\n",
    "        dist=6.8-6.8\n",
    "    if(lay==2) :\n",
    "        dist=10.2-6.8\n",
    "    if(lay==3) :\n",
    "        dist=16-6.8\n",
    "    distx=dist/0.01\n",
    "    disty=dist/0.015\n",
    "\n",
    "    y_out = disty*math.sin(theta-theta_jet)/(math.sin(theta_jet)*math.sin(theta))+y1\n",
    "    x_out = distx*math.tan(-phi)+x1\n",
    "\n",
    "    return (x_out,y_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = True\n",
    "PREDICT = False\n",
    "ON_DATA = False\n",
    "DRAW_ONLY = False\n",
    "RGB = True\n",
    "\n",
    "valSplit=0.2\n",
    "numPrint =1 #number of event saved in the root file\n",
    "outEvent= 30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_ = chunk[0][0]\n",
    "input_jeta = chunk[0][1]\n",
    "input_jpt = chunk[0][2]\n",
    "target_ = chunk[1][0]\n",
    "target_prob = chunk[1][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT :\n",
    "    import ROOT\n",
    "    from ROOT import gStyle\n",
    "    from ROOT import gROOT, TColor, TAxis\n",
    "    gROOT.Reset()\n",
    "    gROOT.SetBatch(True)\n",
    "    gStyle.SetOptStat(0)\n",
    "    validation_offset=int(len(input_)*(1-valSplit)+1)\n",
    "\n",
    "    canvasTot = []\n",
    "    canvasProb = []\n",
    "\n",
    "    mapTot = []\n",
    "    graphTargetTot = []\n",
    "    mapProbPredTot = []\n",
    "    graphPredTot = []\n",
    "\n",
    "\n",
    "    for jet in range(numPrint) :\n",
    "\n",
    "        canvasTot_jet = []\n",
    "        mapTot_jet = []\n",
    "        graphTargetTot_jet = []\n",
    "        canvasProb_jet =[]\n",
    "        mapProbPredTot_jet =[]\n",
    "        graphPredTot_jet = []\n",
    "\n",
    "\n",
    "        for trk in range(overlapNum) :\n",
    "            canvasProb_jet.append(ROOT.TCanvas(\"canvasProb_%d_%d\" % (jet,trk), \"canvasProb_%d_%d\" % (jet,trk),500,800))\n",
    "            mapProbPredTot_jet.append(ROOT.TH2F(\"mapProbPredTot_%d_%d\" % (jet,trk), \"mapProbPredTot_%d_%d\" % (jet,trk), jetDim,-jetDim/2,jetDim/2,jetDim,-jetDim/2,jetDim/2))\n",
    "\n",
    "        for lay in range(layNum) :\n",
    "            mapTot_jet.append(ROOT.TH2F(\"mapTot_%d_%d\" % (jet, lay), \"mapTot_%d_%d\" % (jet, lay), jetDim,-jetDim/2,jetDim/2,jetDim,-jetDim/2,jetDim/2))\n",
    "            canvasTot_jet.append(ROOT.TCanvas(\"canvasTot_%d_%d\" % (jet, lay), \"canvasTot_%d_%d\" % (jet, lay),500,800))\n",
    "            graphTargetTot_jet.append(ROOT.TGraph())\n",
    "            #  graphPredTot_jet.append(ROOT.TGraph(overlapNum*3))\n",
    "            graphPredTot_jet.append(ROOT.TGraph())\n",
    "\n",
    "        mapTot.append(mapTot_jet)\n",
    "        \n",
    "        canvasTot.append(canvasTot_jet)\n",
    "        graphTargetTot.append(graphTargetTot_jet)\n",
    "        mapProbPredTot.append(mapProbPredTot_jet)\n",
    "        canvasProb.append(canvasProb_jet)\n",
    "        graphPredTot.append(graphPredTot_jet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for jet in range(numPrint) :\n",
    "        print(\"=================================== New Event ======================================\")\n",
    "\n",
    "        j_eff = jet+validation_offset\n",
    "        #  j_eff = jet #WARNING, is this intended? \n",
    "\n",
    "        #check if lay1 is broken\n",
    "        brokenLay_flag = False\n",
    "        brokenLay_cut = 0\n",
    "        for x in range(jetDim) :\n",
    "            for y in range(jetDim) :\n",
    "                if(input_[j_eff][x][y][1] > 0.0001) :\n",
    "                    brokenLay_flag = True\n",
    "        if(not brokenLay_flag) :\n",
    "            brokenLay_cut = 0.35\n",
    "\n",
    "         # fill the histos   \n",
    "        for lay in range(layNum) :\n",
    "            tarPoint = 0\n",
    "            predPoint = 0\n",
    "            graphPredTot[jet][lay].SetMarkerColor(7)\n",
    "            graphPredTot[jet][lay].SetMarkerStyle(28)\n",
    "            graphPredTot[jet][lay].SetMarkerSize(3)\n",
    "            graphTargetTot[jet][lay].SetMarkerColor(6)\n",
    "            graphTargetTot[jet][lay].SetMarkerStyle(2)\n",
    "            graphTargetTot[jet][lay].SetMarkerSize(3)\n",
    "            for x in range(jetDim) :\n",
    "                for y in range(jetDim) :\n",
    "                    mapTot[jet][lay].SetBinContent(x+1,y+1,input_[j_eff][x][y][lay])\n",
    "                    if(input_[j_eff][x][y][lay]>0) : print(\"input pixel:\", \"(x,y)=\",x,y, \", layer=\",lay, \", value=\", input_[j_eff][x][y][lay])\n",
    "                    for trk in range(overlapNum) :\n",
    "                        if not DRAW_ONLY : \n",
    "                            mapProbPredTot[jet][trk].SetBinContent(x+1,y+1,validation_prob[j_eff][x][y][trk][0])\n",
    "                        if(not ON_DATA) :\n",
    "                            if target_prob[j_eff][x][y][trk][0] == 1 and lay==1: # check this\n",
    "                                xx= float(target_[j_eff][x][y][trk][0])/float(0.01)*0.01#normaliz. factor\n",
    "                                yy= float(target_[j_eff][x][y][trk][1])/float(0.015)*0.01\n",
    "                                graphTargetTot[jet][lay].SetPoint(tarPoint,x+xx-jetDim/2,y+yy-jetDim/2)\n",
    "\n",
    "                                x0,y0 = prop_on_layer(x+xx-jetDim/2, y+yy-jetDim/2,target_[j_eff][x][y][trk][2]*0.01,target_[j_eff][x][y][trk][3]*0.01,input_jeta[j_eff],0)\n",
    "                                x2,y2 = prop_on_layer(x+xx-jetDim/2, y+yy-jetDim/2,target_[j_eff][x][y][trk][2]*0.01,target_[j_eff][x][y][trk][3]*0.01,input_jeta[j_eff],2)\n",
    "                                x3,y3 = prop_on_layer(x+xx-jetDim/2, y+yy-jetDim/2,target_[j_eff][x][y][trk][2]*0.01,target_[j_eff][x][y][trk][3]*0.01,input_jeta[j_eff],3)\n",
    "                                graphTargetTot[jet][0].SetPoint(tarPoint,x0,y0)\n",
    "                                graphTargetTot[jet][2].SetPoint(tarPoint,x2,y2)\n",
    "                                graphTargetTot[jet][3].SetPoint(tarPoint,x3,y3)\n",
    "                                tarPoint = tarPoint+1\n",
    "                        if not DRAW_ONLY :\n",
    "                            if validation_prob[j_eff][x][y][trk][0] > (prob_thr-0.1*trk-brokenLay_cut) and lay==1 : #and   target_prob[j_eff][x][y][trk] == 1: #this is an useful option to debug\n",
    "                                xx_pr= float(validation_par[j_eff][x][y][trk][0])/float(0.01)*0.01\n",
    "                                yy_pr= float(validation_par[j_eff][x][y][trk][1])/float(0.015)*0.01\n",
    "                                graphPredTot[jet][lay].SetPoint(predPoint,x+xx_pr-jetDim/2,y+yy_pr-jetDim/2)\n",
    "\n",
    "                                x0,y0 = prop_on_layer(x+xx_pr-jetDim/2, y+yy_pr-jetDim/2,validation_par[j_eff][x][y][trk][2]*0.01,validation_par[j_eff][x][y][trk][3]*0.01,input_jeta[j_eff],0)\n",
    "                                x2,y2 = prop_on_layer(x+xx_pr-jetDim/2, y+yy_pr-jetDim/2,validation_par[j_eff][x][y][trk][2]*0.01,validation_par[j_eff][x][y][trk][3]*0.01,input_jeta[j_eff],2)\n",
    "                                x3,y3 = prop_on_layer(x+xx_pr-jetDim/2, y+yy_pr-jetDim/2,validation_par[j_eff][x][y][trk][2]*0.01,validation_par[j_eff][x][y][trk][3]*0.01,input_jeta[j_eff],3)\n",
    "                                graphPredTot[jet][0].SetPoint(predPoint,x0,y0)\n",
    "                                graphPredTot[jet][2].SetPoint(predPoint,x2,y2)\n",
    "                                graphPredTot[jet][3].SetPoint(predPoint,x3,y3)\n",
    "                                predPoint = predPoint+1\n",
    "\n",
    "                                print(\"________________________________________\")\n",
    "                                print(\"New Pred, bin (x,y):\",x-jetDim/2,y-jetDim/2)\n",
    "                                if(not ON_DATA):\n",
    "                                    print(\"target(x,y,eta,phi)=\",target_[j_eff][x][y][trk][0],\" \", target_[j_eff][x][y][trk][1],\" \",target_[j_eff][x][y][trk][2],\" \",target_[j_eff][x][y][trk][3],\" \",target_[j_eff][x][y][trk][4],\"Probabiity target=\", target_prob[j_eff][x][y][trk])\n",
    "                                    print(\"prediction(x,y,eta,phi)=\",validation_par[j_eff][x][y][trk][0],\" \", validation_par[j_eff][x][y][trk][1],\" \",validation_par[j_eff][x][y][trk][2],\" \",validation_par[j_eff][x][y][trk][3],\" \",validation_par[j_eff][x][y][trk][4], \"Probabiity pred=\", validation_prob[j_eff][x][y][trk])\n",
    "                                    print(\" x0,y0=\",x0,y0,\" x2,y2=\",x2,y2,\" x3,y3=\",x3,y3,)\n",
    "\n",
    "\n",
    "    output_file = ROOT.TFile(\"DeepCore_mapValidation.root\",\"recreate\")\n",
    "    from array import array as array2\n",
    "\n",
    "    if(RGB) : #set the color scheme\n",
    "\n",
    "        NCont=10\n",
    "\n",
    "        array_of_palette = []\n",
    "        palette = []\n",
    "\n",
    "        Red =[1.,1.]\n",
    "        Green =[1.,0.]\n",
    "        Blue =[1.,0.]\n",
    "        Stops =[0.,1.]\n",
    "        StopsArray = array2('d', Stops)\n",
    "        RedArray = array2('d', Red)\n",
    "        GreenArray = array2('d', Green)\n",
    "        BlueArray = array2('d', Blue)\n",
    "        FI = TColor.CreateGradientColorTable(2, StopsArray, RedArray, GreenArray, BlueArray, NCont)\n",
    "        for i in range(0,NCont) :\n",
    "            palette.append(FI+i)\n",
    "        paletteArray = array2('i',palette)\n",
    "        palette[:]=[]\n",
    "        array_of_palette.append(paletteArray)\n",
    "\n",
    "\n",
    "        Red =[1.,0.]\n",
    "        Green =[1.,0.]\n",
    "        Blue =[1.,0.]\n",
    "        Stops =[0.,1.]\n",
    "        StopsArray = array2('d', Stops)\n",
    "        RedArray = array2('d', Red)\n",
    "        GreenArray = array2('d', Green)\n",
    "        BlueArray = array2('d', Blue)\n",
    "        FI = TColor.CreateGradientColorTable(2, StopsArray, RedArray, GreenArray, BlueArray, NCont)\n",
    "        for i in range(0,NCont) :\n",
    "            palette.append(FI+i)\n",
    "        paletteArray = array2('i',palette)\n",
    "        palette[:]=[]\n",
    "        array_of_palette.append(paletteArray)\n",
    "\n",
    "        Red =[1.,0]\n",
    "        Green =[1.,1]\n",
    "        Blue =[1.,0.]\n",
    "        Stops =[0.,1.]\n",
    "        StopsArray = array2('d', Stops)\n",
    "        RedArray = array2('d', Red)\n",
    "        GreenArray = array2('d', Green)\n",
    "        BlueArray = array2('d', Blue)\n",
    "        FI = TColor.CreateGradientColorTable(2, StopsArray, RedArray, GreenArray, BlueArray, NCont)\n",
    "        for i in range(0,NCont) :\n",
    "            palette.append(FI+i)\n",
    "        paletteArray = array2('i',palette)\n",
    "        palette[:]=[]\n",
    "        array_of_palette.append(paletteArray)\n",
    "\n",
    "        Red =[1.,0.]\n",
    "        Green =[1.,0.]\n",
    "        Blue =[1.,1.]\n",
    "        Stops =[0.,1.]\n",
    "        StopsArray = array2('d', Stops)\n",
    "        RedArray = array2('d', Red)\n",
    "        GreenArray = array2('d', Green)\n",
    "        BlueArray = array2('d', Blue)\n",
    "        FI = TColor.CreateGradientColorTable(2, StopsArray, RedArray, GreenArray, BlueArray, NCont)\n",
    "        for i in range(0,NCont) :\n",
    "            palette.append(FI+i)\n",
    "        paletteArray = array2('i',palette)\n",
    "        palette[:]=[]\n",
    "        array_of_palette.append(paletteArray)\n",
    "\n",
    "     # build the canvases\n",
    "    for jet in range(numPrint) :\n",
    "\n",
    "         #check if lay1 is broken!\n",
    "        brokenLay_flag = False\n",
    "        brokenLay_cut = 0\n",
    "        for x in range(jetDim) :\n",
    "            for y in range(jetDim) :\n",
    "                if(input_[jet][x][y][1] > 0.0001) :\n",
    "                    brokenLay_flag = True\n",
    "        if(not brokenLay_flag) :\n",
    "            brokenLay_cut = 0.35\n",
    "        \n",
    "\n",
    "        for lay in range(layNum) :\n",
    "            canvasTot[jet][lay].cd()\n",
    "            mapTot[jet][lay].GetXaxis().SetRangeUser(-jetDim,jetDim)\n",
    "            mapTot[jet][lay].GetYaxis().SetRangeUser(-jetDim,jetDim)\n",
    "            #  mapTot[jet][lay].SetTitle(\"Pixel Map, cluster %d, layer %d, pt=%f, eta=%f\" % (jet, lay+1, input_jpt[jet],input_jeta[jet]))\n",
    "            mapTot[jet][lay].SetTitle(\"Pixel Window, layer %d\" % (lay+1))\n",
    "            mapTot[jet][lay].GetXaxis().SetTitle(\"x [pixel]\")\n",
    "            mapTot[jet][lay].GetYaxis().SetTitle(\"y [pixel]\")\n",
    "            mapTot[jet][lay].GetYaxis().SetTitleOffset(1)\n",
    "            mapTot[jet][lay].GetZaxis().SetTitle(\"ADC count [/14k]\")\n",
    "            mapTot[jet][lay].GetZaxis().SetTitleOffset(-1.05)\n",
    "            mapTot[jet][lay].GetXaxis().SetTitleSize(0.06)\n",
    "            mapTot[jet][lay].GetYaxis().SetTitleSize(0.06)\n",
    "            mapTot[jet][lay].GetZaxis().SetTitleSize(0.04)\n",
    "            mapTot[jet][lay].GetXaxis().SetTitleOffset(0.7)\n",
    "            mapTot[jet][lay].GetYaxis().SetTitleOffset(0.6)\n",
    "\n",
    "            latexCMS = ROOT.TLatex()\n",
    "\n",
    "\n",
    "            if(not RGB) :\n",
    "                mapTot[jet][lay].Draw(\"colz\")\n",
    "\n",
    "                latexCMS.SetTextSize(0.05)\n",
    "                latexCMS.DrawLatex(-16.2,16.2,\"#bf{#bf{CMS}} #scale[0.7]{#bf{#it{Simulation Preliminary}}}\")\n",
    "                latexCMS.DrawLatex(12,16.2,\"#bf{13 TeV}\")\n",
    "\n",
    "            else :\n",
    "                gStyle.SetPalette(NCont,array_of_palette[lay])\n",
    "                mapTot[jet][lay].Draw(\"colz\")\n",
    "\n",
    "                latexCMS.SetTextSize(0.05)\n",
    "                latexCMS.DrawLatex(-16.2,16.2,\"#bf{#bf{CMS}} #scale[0.7]{#bf{#it{Simulation Preliminary}}}\")\n",
    "                latexCMS.DrawLatex(12,16.2,\"#bf{13 TeV}\")\n",
    "\n",
    "            if(jet==outEvent and RGB):\n",
    "\n",
    "\n",
    "                mapTot[jet][lay].GetZaxis().SetRangeUser(0,2.7)\n",
    "                canvasTot[jet][lay].SaveAs(\"RGB_PixelMap_input_layer%d_event%d.pdf\" % (lay,jet))\n",
    "                canvasTot[jet][lay].SaveAs(\"RGB_PixelMap_input_layer%d_event%d.png\" % (lay,jet))\n",
    "\n",
    "            if (not ON_DATA) :\n",
    "                graphTargetTot[jet][lay].Draw(\"SAME P\")\n",
    "            graphPredTot[jet][lay].Draw(\"SAME P\")\n",
    "\n",
    "            graphTargetTot[jet][lay].SetLineColor(0)\n",
    "            graphPredTot[jet][lay].SetLineColor(0)\n",
    "            graphTargetTot[jet][lay].SetFillColor(0)\n",
    "            graphPredTot[jet][lay].SetFillColor(0)\n",
    "\n",
    "            latexCMS.SetTextSize(0.05)\n",
    "            latexCMS.DrawLatex(-16.2,16.2,\"#bf{#bf{CMS}} #scale[0.7]{#bf{#it{Simulation Preliminary}}}\")\n",
    "            latexCMS.DrawLatex(12,16.2,\"#bf{13 TeV}\")\n",
    "\n",
    "\n",
    "\n",
    "            legTot = ROOT.TLegend(0.1,0.9,0.3,0.8);\n",
    "            if (not ON_DATA) :\n",
    "                legTot.AddEntry(graphTargetTot[jet][lay], \"Target\")\n",
    "            legTot.AddEntry(graphPredTot[jet][lay], \"Prediction\")\n",
    "            legTot.SetTextSize(0.03);\n",
    "            legTot.Draw(\"SAME\")\n",
    "\n",
    "            canvasTot[jet][lay].Write()\n",
    "\n",
    "            if(jet==outEvent and RGB):\n",
    "                canvasTot[jet][lay].SaveAs(\"RGB_PixelMap_crosses_layer%d_event%d.pdf\" % (lay,jet))\n",
    "                canvasTot[jet][lay].SaveAs(\"RGB_PixelMap_crosses_layer%d_event%d.png\" % (lay,jet))\n",
    "\n",
    "        for trk in range(overlapNum) :\n",
    "            canvasProb[jet][trk].cd()\n",
    "            mapProbPredTot[jet][trk].GetXaxis().SetRangeUser(-jetDim,jetDim)\n",
    "            mapProbPredTot[jet][trk].GetYaxis().SetRangeUser(-jetDim,jetDim)\n",
    "            gStyle.SetPalette(57)\n",
    "            mapProbPredTot[jet][trk].Draw(\"colz\")\n",
    "            mapProbPredTot[jet][trk].GetXaxis().SetTitle(\"x [pixel]\")\n",
    "            mapProbPredTot[jet][trk].GetYaxis().SetTitle(\"y [pixel]\")\n",
    "            mapProbPredTot[jet][trk].GetYaxis().SetTitleOffset(1.2)\n",
    "            #  mapProbPredTot[jet][trk].SetTitle(\"TCP Prediction Map, cluster %d, overlap %d\" % (jet, trk))\n",
    "            mapProbPredTot[jet][trk].SetTitle(\"TCP Prediction Map, overlap %d\" % (trk))\n",
    "            mapProbPredTot[jet][trk].GetZaxis().SetTitle(\"Probability\")\n",
    "            mapProbPredTot[jet][trk].GetZaxis().SetTitleOffset(-1.05)\n",
    "            mapProbPredTot[jet][trk].GetXaxis().SetTitleSize(0.06)\n",
    "            mapProbPredTot[jet][trk].GetYaxis().SetTitleSize(0.06)\n",
    "            mapProbPredTot[jet][trk].GetXaxis().SetTitleOffset(0.7)\n",
    "            mapProbPredTot[jet][trk].GetYaxis().SetTitleOffset(0.6)\n",
    "            if (not ON_DATA) :\n",
    "                graphTargetTot[jet][1].Draw(\"SAME P\")\n",
    "            graphPredTot[jet][1].Draw(\"SAME P\")\n",
    "\n",
    "            latexCMS.SetTextSize(0.05)\n",
    "            latexCMS.DrawLatex(-16.2,16.2,\"#bf{#bf{CMS}} #scale[0.7]{#bf{#it{Simulation Preliminary}}}\")\n",
    "            latexCMS.DrawLatex(12,16.2,\"#bf{13 TeV}\")\n",
    "\n",
    "\n",
    "            legProb = ROOT.TLegend(0.1,0.9,0.3,0.8);\n",
    "            if (not ON_DATA) :\n",
    "                legProb.AddEntry(graphTargetTot[jet][1], \"Target\")\n",
    "            legProb.AddEntry(graphPredTot[jet][1], \"Prediction\")\n",
    "            legProb.SetTextSize(0.03);\n",
    "            legProb.Draw(\"SAME\")\n",
    "\n",
    "            mapProbPredTot[jet][0].GetZaxis().SetRangeUser(0,1)\n",
    "            canvasProb[jet][trk].Write()\n",
    "\n",
    "        if(jet==outEvent and RGB):\n",
    "            canvasProb[jet][0].SaveAs(\"Probabiltiy_crosses_event%d.pdf\" % (jet))#.png\n",
    "            canvasProb[jet][0].SaveAs(\"Probabiltiy_crosses_event%d.png\" % (jet))#.png\n",
    "\n",
    "    output_file.Close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca000f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "file_path = \"DeepCore_mapValidation.root\"\n",
    "\n",
    "\n",
    "root_file = ROOT.TFile.Open(file_path, \"READ\")\n",
    "\n",
    "# Check if the file was opened successfully\n",
    "if not root_file or root_file.IsZombie():\n",
    "    print(f\"Error: Unable to open file '{file_path}'\")\n",
    "        \n",
    "# Get the list of keys (objects) in the file\n",
    "list_of_keys = root_file.GetListOfKeys()\n",
    "\n",
    "# Loop over the keys\n",
    "for key in list_of_keys:\n",
    "    obj = key.ReadObj()\n",
    "    if isinstance(obj, ROOT.TCanvas):\n",
    "        # Display the canvas\n",
    "        obj.Draw()\n",
    "        obj.SaveAs(obj.GetName() + \".png\")\n",
    "        display(Image(obj.GetName() + \".png\"))\n",
    "\n",
    "# Keep canvases displayed until the user closes them\n",
    "#ROOT.gApplication.Run()\n",
    "\n",
    "#display(Image(\"hist.png\"))\n",
    "# Close the file when you are done\n",
    "root_file.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181128b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36d318d7",
   "metadata": {},
   "source": [
    "# More Stats Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346471d8",
   "metadata": {},
   "source": [
    "Here we plot score plots and parameters again, but we use more stats (up to x100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7891ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import metrics\n",
    "\n",
    "def ROC_plot(truth, pred): #Make a ROC plot\n",
    "    x = [0,1]\n",
    "    fpr_keras, tpr_keras, _ = metrics.roc_curve(truth, pred)\n",
    "    auc = np.trapz(tpr_keras,fpr_keras)\n",
    "    #print auc\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc))\n",
    "    plt.plot(x, x, color='red')  \n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC Curve (area = {:.3f})'.format(auc))\n",
    "    plt.savefig(\"ROC.png\")\n",
    "    plt.savefig(\"ROC.pdf\")\n",
    "    plt.show() \n",
    "    \n",
    "def Score_plot(signal, background, bin_size = 1000, normalization = False, logscale = True): # make a prediction score plot\n",
    "    plt.hist(signal, alpha = 0.5, color = 'b', label = 'Signal', range = (0,1), bins = bin_size, density = normalization)\n",
    "    plt.hist(background, color = 'r', alpha = 0.5, label = 'Background', range = (0,1), bins = bin_size, density = normalization)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Prediction Score')\n",
    "    plt.ylabel('Predicted TCPs')\n",
    "    if logscale:\n",
    "        plt.yscale('log')\n",
    "    plt.title('Prediction Score Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "def Score_save(N, signal, background, l=0, truth=False, pred=False):\n",
    "#def Score_save(signal, background, N , l): # This version of \n",
    "    Nx = 30 # X dimension\n",
    "    Ny = 30 # Y dimension\n",
    "   # Ntrk = 3 # Number of tracks\n",
    "    s = 0 # s is the number of merged clusters that passed the selection (e.g: layer2(adc)> 0)\n",
    "\n",
    "    for i in range(N):\n",
    "        A = np.array(chunk[0][0][i])\n",
    "        S= np.sum(A, axis=(0,1))\n",
    "        # Selection \n",
    "        if S[1] > 0 : # if sum of adc in layer2 is positive then ignore this merged cluster\n",
    "       # if S[1] == 0 or S[3]> 0 or S[2]> 0:  # if sum of adc in layer2 is 0 or sum of adc in layer4 or layer3 is positive then ignore this merged cluster\n",
    "       # if S[0]*S[2]*S[3]== 0: # if at least one of the sum of adc in layer1, layer3 or layer4 is 0, then ignore this merged cluster\n",
    "            continue\n",
    "        s = s +1\n",
    "        for j in range(Nx):\n",
    "            for k in range(Ny):\n",
    "               # for l in range(Ntrk) :\n",
    "                    #l = 0  #to look at Overlap maps, just set l = 0/1/2 here\n",
    "                    pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                    pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                    if (truth and pred) or s ==1:\n",
    "                        truth.append(pixel_true_prob)\n",
    "                        pred.append(pixel_pred_prob)\n",
    "                    if pixel_true_prob ==1 :\n",
    "                        signal.append(pixel_pred_prob)\n",
    "                    else:\n",
    "                        background.append(pixel_pred_prob)\n",
    "    return s \n",
    "\n",
    "\n",
    "# populates the arrays with parameter values for plotting\n",
    "def set_params(chunk, validation_par,jet_eta, jet_pt, targ_dx,pred_dx,targ_dy,pred_dy,targ_deta,pred_deta,targ_dphi,pred_dphi, targ_pt,pred_pt):\n",
    "    Nx = 30 # X dimension\n",
    "    Ny = 30 # Y dimension\n",
    "    Ntrk = 3 # Number of tracks\n",
    "    N = len(chunk[1][0])\n",
    "    for i in range(N):\n",
    "        # using the DeepCore threshold used above\n",
    "        A = np.array(chunk[0][0][i])\n",
    "        S= np.sum(A, axis=(0,1))\n",
    "        dth_l134 = 1\n",
    "        dth_l2 = 1\n",
    "        if (S[0]*S[2]*S[3]): #less info, threshold raised as we know deepcore does worse\n",
    "            dth_l134 = 0\n",
    "        if (S[1]): # if layer 2 empty, should not predict - how can it make prediction without charge\n",
    "            dth_l2 = 0\n",
    "        # Selection \n",
    "        \n",
    "        for j in range(Nx):\n",
    "            for k in range(Ny):\n",
    "                for l in range(Ntrk) :\n",
    "                    #l = 0  #to look at Overlap maps, just set l = 0/1/2 here\n",
    "                    pixel_true_prob = chunk[1][1][i][j][k][l][0]\n",
    "                    pixel_pred_prob = validation_prob[i,j,k,l,0]\n",
    "                    pixel_lay2_adc = chunk[0][0][i][j][k][1]\n",
    "                    if (pixel_pred_prob > (prob_threshold[l] + dth_l134*prob_dth_l134[l] + dth_l2*prob_dth_l2[l] )) and (pixel_true_prob == 1):\n",
    "                        targ_dx.append(chunk[1][0][i][j][k][l][0])\n",
    "                        pred_dx.append(validation_par[i,j,k,l,0])\n",
    "                        #targ_dy.append(chunk[1][0][i][j][k][l][1])\n",
    "                        #pred_dy.append(validation_par[i,j,k,l,1])\n",
    "                        #targ_deta.append(chunk[1][0][i][j][k][l][2])\n",
    "                        #pred_deta.append(validation_par[i,j,k,l,2])\n",
    "                        #targ_dphi.append(chunk[1][0][i][j][k][l][3])\n",
    "                        #pred_dphi.append(validation_par[i,j,k,l,3])\n",
    "                        #targ_pt.append(1/chunk[1][0][i][j][k][l][4])\n",
    "                        #pred_pt.append(1/validation_par[i,j,k,l,4])\n",
    "                        #jet_eta.append(chunk[0][1][i])\n",
    "                        #jet_pt.append(chunk[0][2][i])\n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abae5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 68ms/step\n",
      "i =  0\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "7/7 [==============================] - 0s 64ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 55ms/step\n",
      "12/12 [==============================] - 1s 56ms/step\n",
      "12/12 [==============================] - 1s 56ms/step\n",
      "10/10 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 56ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "8/8 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "10/10 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "8/8 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "i =  100\n",
      "11/11 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "6/6 [==============================] - 0s 57ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "9/9 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "10/10 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "11/11 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "9/9 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "i =  200\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "11/11 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "10/10 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "5/5 [==============================] - 0s 60ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "7/7 [==============================] - 0s 67ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "9/9 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "i =  300\n",
      "12/12 [==============================] - 1s 73ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "11/11 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 81ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "10/10 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "10/10 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 73ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "10/10 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "8/8 [==============================] - 0s 62ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "8/8 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "10/10 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 78ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 73ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 88ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "9/9 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "11/11 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "i =  400\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "9/9 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 80ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "7/7 [==============================] - 0s 68ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "11/11 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "9/9 [==============================] - 1s 57ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "8/8 [==============================] - 0s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "10/10 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 78ms/step\n",
      "10/10 [==============================] - 1s 73ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "9/9 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "10/10 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 82ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "i =  500\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "9/9 [==============================] - 1s 93ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 89ms/step\n",
      "12/12 [==============================] - 1s 85ms/step\n",
      "12/12 [==============================] - 1s 83ms/step\n",
      "12/12 [==============================] - 1s 78ms/step\n",
      "12/12 [==============================] - 1s 79ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "5/5 [==============================] - 0s 69ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "8/8 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "7/7 [==============================] - 0s 60ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 58ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "5/5 [==============================] - 0s 61ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "10/10 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 81ms/step\n",
      "12/12 [==============================] - 1s 88ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "2/2 [==============================] - 0s 42ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 2s 179ms/step\n",
      "12/12 [==============================] - 2s 115ms/step\n",
      "10/10 [==============================] - 1s 102ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 85ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "7/7 [==============================] - 0s 71ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "i =  600\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 72ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 90ms/step\n",
      "12/12 [==============================] - 1s 80ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "7/7 [==============================] - 0s 64ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "5/5 [==============================] - 0s 52ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 85ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "10/10 [==============================] - 1s 88ms/step\n",
      "12/12 [==============================] - 1s 101ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "10/10 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 82ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 81ms/step\n",
      "12/12 [==============================] - 1s 73ms/step\n",
      "12/12 [==============================] - 2s 155ms/step\n",
      "12/12 [==============================] - 1s 96ms/step\n",
      "8/8 [==============================] - 1s 93ms/step\n",
      "12/12 [==============================] - 1s 80ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 81ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "8/8 [==============================] - 0s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "8/8 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 85ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 83ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "7/7 [==============================] - 0s 69ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "i =  700\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "8/8 [==============================] - 1s 81ms/step\n",
      "12/12 [==============================] - 1s 93ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "9/9 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "9/9 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "9/9 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "5/5 [==============================] - 0s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "9/9 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "10/10 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "10/10 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "4/4 [==============================] - 0s 58ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "i =  800\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "5/5 [==============================] - 0s 62ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "8/8 [==============================] - 0s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "11/11 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "7/7 [==============================] - 0s 59ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 79ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "8/8 [==============================] - 0s 60ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 73ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "9/9 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "6/6 [==============================] - 0s 68ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 81ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 99ms/step\n",
      "12/12 [==============================] - 1s 90ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "5/5 [==============================] - 0s 52ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 103ms/step\n",
      "12/12 [==============================] - 2s 112ms/step\n",
      "12/12 [==============================] - 1s 84ms/step\n",
      "12/12 [==============================] - 1s 79ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 80ms/step\n",
      "12/12 [==============================] - 1s 93ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "10/10 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "i =  900\n",
      "8/8 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 84ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 71ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 98ms/step\n",
      "12/12 [==============================] - 1s 68ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "9/9 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "8/8 [==============================] - 0s 63ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "8/8 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 70ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 77ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "8/8 [==============================] - 0s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 75ms/step\n",
      "12/12 [==============================] - 1s 76ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "3/3 [==============================] - 0s 43ms/step\n",
      "12/12 [==============================] - 1s 63ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 61ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 74ms/step\n",
      "11/11 [==============================] - 1s 69ms/step\n",
      "12/12 [==============================] - 1s 82ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 64ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 73ms/step\n",
      "12/12 [==============================] - 1s 72ms/step\n",
      "12/12 [==============================] - 1s 66ms/step\n",
      "8/8 [==============================] - 1s 80ms/step\n",
      "12/12 [==============================] - 1s 67ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 60ms/step\n",
      "12/12 [==============================] - 1s 59ms/step\n",
      "12/12 [==============================] - 1s 62ms/step\n",
      "12/12 [==============================] - 1s 79ms/step\n",
      "Number of Merged clusters:  345019\n",
      "Number of Merged clusters that pass our selection:  59625\n",
      "Fraction:  17.282  %\n"
     ]
    }
   ],
   "source": [
    "# Loading DeepCore model and evaluation of ALL test files\n",
    "model.load_weights('/Users/liamoshaughnessy/Desktop/Research_2023_2024/Code/Python_Files/Semester_1/DeepCore_model_0622.h5') # DeepCore 2.2\n",
    "inputs = []\n",
    "#Lists for Overlap 1\n",
    "signal_a = []\n",
    "background_a = []\n",
    "truth_a = []\n",
    "pred_a = []\n",
    "#Lists for Overlap 2 \n",
    "signal_b = []\n",
    "background_b = []\n",
    "truth_b = []\n",
    "pred_b = []\n",
    "#Lists for Overlap 3\n",
    "signal_c = []\n",
    "background_c = []\n",
    "truth_c = []\n",
    "pred_c = []\n",
    "\n",
    "\n",
    "# lists to hold values of parameters\n",
    "targ_dx=[]\n",
    "pred_dx=[]\n",
    "targ_dy=[]\n",
    "pred_dy=[]\n",
    "targ_deta=[]\n",
    "pred_deta=[]\n",
    "targ_dphi=[]\n",
    "pred_dphi=[]\n",
    "targ_pt=[]\n",
    "pred_pt=[]\n",
    "jet_eta=[]\n",
    "jet_pt=[]\n",
    "\n",
    "\n",
    "\n",
    "# init counters\n",
    "N = 0\n",
    "Nadc = 0\n",
    "#init the first chunk\n",
    "genObject = Generator2(testing_path,batch) \n",
    "# Total number of merged clusters is 300k, and each chunk is a little less than 300, so to look over all stats you set ST = 1000\n",
    "ST = 1000 #smaller\n",
    "\n",
    "for i in range(ST):\n",
    "    chunk=next(genObject)\n",
    "    #chunk = next(Generator2(testing_path,batch))\n",
    "    [validation_par,validation_prob] = model.predict(chunk[0])\n",
    "    validation_par = np.float64(validation_par)\n",
    "    set_params(chunk, validation_par,jet_eta, jet_pt, targ_dx,pred_dx,targ_dy,pred_dy,targ_deta,pred_deta,targ_dphi,pred_dphi, targ_pt,pred_pt)\n",
    "    #Ni = len(chunk[0][1])\n",
    "    #print(Ni)\n",
    "    #N = N + Ni\n",
    "    # Looking at ovelap 1, can be changed, you can also omit truth and pred to save memory\n",
    "    #NiAdc = Score_save(Ni, signal_a, background_a, 0, truth_a, pred_a) \n",
    "    # You could save Overlap 2 and 3 info in the same loop, but it will take more memory so not recommended. \n",
    "    #Score_save(Ni,, signal_b, background_b, 1 truth_b, pred_b)\n",
    "    #Score_save(Ni, signal_c, background_c, 2 truth_c, pred_c)\n",
    "    #print(NiAdc)\n",
    "    #Nadc = Nadc + NiAdc\n",
    "    #print(chunk[0][1][2])\n",
    "    if i%100 == 0: # Small print out to get an idea when this loop will be done running\n",
    "        print(\"i = \",i)\n",
    "\n",
    "#print(\"Number of Merged clusters: \", N)\n",
    "#print(\"Number of Merged clusters that pass our selection: \", Nadc)\n",
    "#print(\"Fraction: \", round(Nadc*100/N,3), \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5336170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Score_plot(signal_a, background_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502273fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROC_plot(truth_a, pred_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab47243",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#plotting parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m targ_dx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(targ_dx)\n\u001b[1;32m      3\u001b[0m pred_dx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(pred_dx)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(targ_dx))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#plotting parameters\n",
    "targ_dx=100*np.array(targ_dx)\n",
    "pred_dx=100*np.array(pred_dx)\n",
    "print(len(targ_dx))\n",
    "#targ_dy=100*np.array(targ_dy)\n",
    "#pred_dy=100*np.array(pred_dy)\n",
    "#targ_deta=.01*np.array(targ_deta)\n",
    "#pred_deta=.01*np.array(pred_deta)\n",
    "#targ_dphi=.01*np.array(targ_dphi)\n",
    "#pred_dphi=.01*np.array(pred_dphi)\n",
    "#targ_pt=np.array(targ_pt)\n",
    "#pred_pt=np.array(pred_pt)\n",
    "#jet_eta=.01*np.array(jet_eta)\n",
    "#jet_pt=np.array(jet_pt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "residual_plots(0,pred_dx-targ_dx)\n",
    "#residual_plots(1,pred_dy-targ_dy)\n",
    "#residual_plots(2,pred_deta-targ_deta)\n",
    "#residual_plots(3,pred_dphi-targ_dphi)\n",
    "\n",
    "target_plots(0,targ_dx)\n",
    "#target_plots(1,targ_dy)\n",
    "#target_plots(2,targ_deta)\n",
    "#target_plots(3,targ_dphi)\n",
    "\n",
    "pred_plots(0,pred_dx)\n",
    "#pred_plots(1,pred_dy)\n",
    "#pred_plots(2,pred_deta)\n",
    "#pred_plots(3,pred_dphi)\n",
    "\n",
    "scatter_plots(0,targ_dx,pred_dx)\n",
    "#scatter_plots(1,targ_dy,pred_dy)\n",
    "#scatter_plots(2,targ_deta,pred_deta)\n",
    "#scatter_plots(3,targ_dphi,pred_dphi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b5342",
   "metadata": {},
   "source": [
    "# Experimental Stuff\n",
    "Ignore the plots below, I'm just trying stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot Threshold plot\n",
    "fpr_keras, tpr_keras, _ = metrics.roc_curve(truth, pred)\n",
    "S = len(signal_a)*tpr_keras\n",
    "B = len(background_a)*fpr_keras\n",
    "metric = S/np.sqrt(S+B)\n",
    "\n",
    "plt.plot(_, metric)\n",
    "xmax = _[np.argmax(metric)]\n",
    "ymax = metric.max()\n",
    "print(xmax, ymax)\n",
    "plt.xlabel('Threshold value')\n",
    "plt.ylabel('$\\\\frac{S}{\\\\sqrt{S+B}}$')\n",
    "plt.xlim(0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3405c",
   "metadata": {},
   "source": [
    "# Random plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21306e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates: not needed here\n",
    "\n",
    "sigf = []\n",
    "for i in range(len(signal)):\n",
    "    x = [signal_target[i][0],signal_target[i][1], signal_target[i][2], signal_target[i][3],signal_target[i][4],signal[i]]\n",
    "    sigf.append(x)\n",
    "    \n",
    "for i in range(20):\n",
    "    print(signal_target[i][4], signal[i])\n",
    "    print(sigf[i])\n",
    "    print(\"*********\")\n",
    "    \n",
    "from operator import itemgetter\n",
    "sigf_sorted = sorted(sigf, key = itemgetter(2))\n",
    "\n",
    "for i in range(50):\n",
    "    print(sigf_sorted[i])\n",
    "\n",
    "sig_nodup = []\n",
    "sig_nodup.append(sigf_sorted[0])\n",
    "for i in range(1,len(sigf_sorted)):\n",
    "        if (sigf_sorted[i][2] == sigf_sorted[i-1][2]):\n",
    "\n",
    "        if not (sigf_sorted[i][2] == sigf_sorted[i-1][2]):\n",
    "            sig_nodup.append(sigf_sorted[i])\n",
    "        \n",
    "for i in range(50):\n",
    "    print(sig_nodup[i])\n",
    "    \n",
    "signal_pred = []\n",
    "for i in range(len(sig_nodup)):\n",
    "        signal_pred.append(sig_nodup[i][5])\n",
    "        \n",
    "print(len(sig_nodup), len(sigf_sorted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing h5 files nodes and weights\n",
    "import h5py\n",
    "hf_in = h5py.File('Valerio_model.hdf5', 'r')\n",
    "hf_in1 = h5py.File('T0628.h5', 'r')\n",
    "\n",
    "print(list(hf_in.keys()))\n",
    "print(list(hf_in1.keys()))\n",
    "\n",
    "model_w = hf_in['model_weights']\n",
    "model_w1 = hf_in1['model_weights']\n",
    "L1 = list(model_w.keys())\n",
    "L2 = list(model_w1.keys())\n",
    "\n",
    "print(\"Valerio_______Hichem\")\n",
    "for i in range(len(L1)):\n",
    "    print(L1[i], L2[i])\n",
    "    \n",
    "model_o = hf_in['optimizer_weights']\n",
    "model_o1 = hf_in1['optimizer_weights']\n",
    "print(list(model_o.keys()))\n",
    "print(list(model_o1.keys()))\n",
    "tra = model_o['training']\n",
    "print(list(tra.keys()))\n",
    "tra2 = tra['Adam']\n",
    "print(list(tra2.keys()))\n",
    "dir(tra2.values())\n",
    "tra2.__format__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5afb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Delta pred(trk(j) - trk(j))\n",
    "signal = []\n",
    "background = []\n",
    "\n",
    "N = 3700 # number of inputs\n",
    "Nx = 30 # X dimension\n",
    "Ny = 30 # Y dimension\n",
    "Ntrk = 3 # Number of tracks\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(Nx):\n",
    "        for k in range(Ny):\n",
    "             if chunk[1][1][i][j][k][0][0] ==0:\n",
    "                signal.append(validation_prob[i,j,k,0,0]-validation_prob[i,j,k,1,0])  \n",
    "                background.append(validation_prob[i,j,k,1,0]-validation_prob[i,j,k,2,0]) \n",
    "               \n",
    "\n",
    "            \n",
    "bin_size = 1000\n",
    "plt.hist(signal, alpha = 0.5, color = 'b', label = '$\\Delta$(trk1 - trk2)', range = (-0.1,0.4), bins = bin_size, density = True)\n",
    "plt.hist(background, alpha = 0.5, color = 'r', label = '$\\Delta$(trk2 - trk3)', range = (-0.1,0.4), bins = bin_size, density = True)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$\\Delta$ Prediction Score ')\n",
    "plt.ylabel('Predicted Tracks')\n",
    "plt.title('$\\Delta$ Prediction Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trk1 score by whether there are 1, 2 or 3 true tracks\n",
    "track_1 = []\n",
    "track_2 = []\n",
    "track_3 = []\n",
    "\n",
    "\n",
    "N = 3700 # number of inputs\n",
    "Nx = 30 # X dimension\n",
    "Ny = 30 # Y dimension\n",
    "Ntrk = 3 # Number of tracks\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(Nx):\n",
    "        for k in range(Ny):\n",
    "                l = 0\n",
    "                if chunk[1][1][i][j][k][2][0] ==1:\n",
    "                    track_3.append(validation_prob[i,j,k,l,0])\n",
    "                elif chunk[1][1][i][j][k][1][0] ==1:\n",
    "                    track_2.append(validation_prob[i,j,k,l,0])\n",
    "                elif chunk[1][1][i][j][k][0][0] ==1:\n",
    "                    track_1.append(validation_prob[i,j,k,l,0])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "bin_size = 1000\n",
    "#bin_size = 50\n",
    "plt.hist(track_3, alpha = 0.5, color = 'b', label = '3 True Tracks', range = (0,1), bins = bin_size, density = True)\n",
    "plt.hist(track_2, alpha = 0.5, color = 'r', label = '2 True Tracks', range = (0,1), bins = bin_size, density = True)\n",
    "plt.hist(track_1, alpha = 0.5, color = 'g', label = '1 True Track', range = (0,1), bins = bin_size, density = True)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Prediction Score')\n",
    "plt.ylabel('Predicted Tracks for Track 1')\n",
    "plt.title('Prediction Score Distribution for Track 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc90b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calulating non charged pixel vs charge pixel ratio in layer 2\n",
    "\n",
    "N = 3700 # number of inputs\n",
    "Nx = 30 # X dimension\n",
    "Ny = 30 # Y dimension\n",
    "adc_0 = 0\n",
    "adc = 0\n",
    "for i in range(N):\n",
    "    for j in range(Nx):\n",
    "        for k in range(Ny):\n",
    "                if chunk[0][0][i][j][k][1] ==0:\n",
    "                    adc_0 = adc_0 + 1\n",
    "                else:\n",
    "                    adc = adc + 1\n",
    "\n",
    "print(adc_0/adc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DeepCore model and evaluation of ALL test files\n",
    "# Alternative method\n",
    "\n",
    "model.load_weights('DeepCore_model_0622.h5') # DeepCore 2.2\n",
    "inputs = []\n",
    "signal = []\n",
    "background = []\n",
    "truth = []\n",
    "pred = []\n",
    "N = 0\n",
    "i = 0\n",
    "i_max = 5\n",
    "chunk = next(Generator2(testing_path,batch,)) \n",
    "\n",
    "for chunk in Generator2(testing_path,batch,):\n",
    "    i = i+1\n",
    "    [validation_par,validation_prob] = model.predict(chunk[0])\n",
    "    validation_par = np.float64(validation_par)\n",
    "    N_i = len(validation_par)\n",
    "    print(N_i)\n",
    "    N = N + N_i\n",
    "    Score_plots(signal, background, truth, pred, N_i, 2)\n",
    "    #print(chunk[0][1][2])\n",
    "   # if i%10 == 0:\n",
    "   #     print(\"i= \",i)\n",
    "    if i > i_max:\n",
    "        break\n",
    "print(len(signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ef7e8",
   "metadata": {},
   "source": [
    "Previous DeepCore models Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af430d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepCore 2.0/2.1 Architecture\n",
    "from keras.layers import AlphaDropout\n",
    "\n",
    "NNinputs_jeta = Input(shape=(1,))\n",
    "NNinputs_jpt = Input(shape=(1,))\n",
    "NNinputsJet = concatenate([NNinputs_jeta,NNinputs_jpt])\n",
    "jetReshaped = Reshape((1,1,2))(NNinputsJet)\n",
    "jetUps = UpSampling2D(size=(jetDim,jetDim), data_format=\"channels_last\")(jetReshaped)\n",
    "print(\"jetUps=\", jetUps.shape)\n",
    "NNinputs = Input(shape=(jetDim,jetDim,layNum))\n",
    "print(\"NNinputs=\", NNinputs.shape)\n",
    "ComplInput = concatenate([NNinputs,jetUps],axis=3)\n",
    "print(\"ComplInput=\", ComplInput.shape)\n",
    "\n",
    "   \n",
    "conv30_9 = Conv2D(50,7, data_format=\"channels_last\", input_shape=(jetDim,jetDim,layNum+2), activation='relu',padding=\"same\")(ComplInput)\n",
    "conv30_7 = Conv2D(40,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_9)\n",
    "conv30_5 = Conv2D(40,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_7)#\n",
    "conv20_5 = Conv2D(30,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_5)\n",
    "conv15_5 = Conv2D(30,3, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv20_5)\n",
    "\n",
    "conv15_3_1 = Conv2D(30,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_5)\n",
    "conv15_3_2 = Conv2D(30,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_3_1)\n",
    "conv15_3_3 = Conv2D(30,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_3_2) #(12,3)\n",
    "conv15_3 = Conv2D(18,3, data_format=\"channels_last\",padding=\"same\")(conv15_3_3) #(12,3)\n",
    "reshaped = Reshape((jetDim,jetDim,overlapNum,parNum+1))(conv15_3)\n",
    "\n",
    "conv12_3_1 = Conv2D(30,3, data_format=\"channels_last\", activation='relu', padding=\"same\")(conv15_5)  #new\n",
    "conv1_3_2 = Conv2D(25,3, data_format=\"channels_last\", activation='relu', padding=\"same\")(conv12_3_1) #drop7lb   #new\n",
    "conv1_3_3 = Conv2D(20,3, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv1_3_2) #new\n",
    "conv1_3_1 = Conv2D(6,3, data_format=\"channels_last\", activation='sigmoid', padding=\"same\")(conv1_3_3)\n",
    "reshaped_prob = Reshape((jetDim,jetDim,overlapNum,2))(conv1_3_1)\n",
    "\n",
    "model = Model([NNinputs,NNinputs_jeta,NNinputs_jpt],[reshaped,reshaped_prob])\n",
    "anubi = tf.keras.optimizers.Adam(learning_rate=0.0000001)#after epochs 252 (with septs/20 and batch_size 64)\n",
    "\n",
    "model.compile(optimizer=anubi, loss=[loss_mse_select_clipped,loss_ROIsoft_crossentropy], loss_weights=[1,1]) #FOR LATE TRAINING\n",
    "#model.compile(optimizer=anubi, loss=[loss_mse_select_clipped,loss_ROI_crossentropy], loss_weights=[1,1]) #FOR EARLY TRAINING\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture DeepCore 1.0\n",
    "from keras.layers import AlphaDropout\n",
    "\n",
    "NNinputs_jeta = Input(shape=(1,))\n",
    "NNinputs_jpt = Input(shape=(1,))\n",
    "NNinputsJet = concatenate([NNinputs_jeta,NNinputs_jpt])\n",
    "jetReshaped = Reshape((1,1,2))(NNinputsJet)\n",
    "jetUps = UpSampling2D(size=(jetDim,jetDim), data_format=\"channels_last\")(jetReshaped)\n",
    "print(\"jetUps=\", jetUps.shape)\n",
    "NNinputs = Input(shape=(jetDim,jetDim,layNum))\n",
    "print(\"NNinputs=\", NNinputs.shape)\n",
    "ComplInput = concatenate([NNinputs,jetUps],axis=3)\n",
    "print(\"ComplInput=\", ComplInput.shape)\n",
    "\n",
    "   \n",
    "conv30_9 = Conv2D(50,7, data_format=\"channels_last\", input_shape=(jetDim,jetDim,layNum+2), activation='relu',padding=\"same\")(ComplInput)\n",
    "conv30_7 = Conv2D(20,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_9)\n",
    "conv30_5 = Conv2D(20,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_7)#\n",
    "conv20_5 = Conv2D(18,5, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv30_5)\n",
    "conv15_5 = Conv2D(18,3, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv20_5)\n",
    "\n",
    "conv15_3_1 = Conv2D(18,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_5)\n",
    "conv15_3_2 = Conv2D(18,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_3_1)\n",
    "conv15_3_3 = Conv2D(18,3, data_format=\"channels_last\",activation='relu', padding=\"same\")(conv15_3_2) #(12,3)\n",
    "conv15_3 = Conv2D(18,3, data_format=\"channels_last\",padding=\"same\")(conv15_3_3) #(12,3)\n",
    "reshaped = Reshape((jetDim,jetDim,overlapNum,parNum+1))(conv15_3)\n",
    "\n",
    "conv12_3_1 = Conv2D(12,3, data_format=\"channels_last\", activation='relu', padding=\"same\")(conv15_5)  #new\n",
    "conv1_3_2 = Conv2D(9,3, data_format=\"channels_last\", activation='relu', padding=\"same\")(conv12_3_1) #drop7lb   #new\n",
    "conv1_3_3 = Conv2D(7,3, data_format=\"channels_last\", activation='relu',padding=\"same\")(conv1_3_2) #new\n",
    "conv1_3_1 = Conv2D(6,3, data_format=\"channels_last\", activation='sigmoid', padding=\"same\")(conv1_3_3)\n",
    "reshaped_prob = Reshape((jetDim,jetDim,overlapNum,2))(conv1_3_1)\n",
    "\n",
    "model = Model([NNinputs,NNinputs_jeta,NNinputs_jpt],[reshaped,reshaped_prob])\n",
    "anubi = tf.keras.optimizers.Adam(learning_rate=0.00001)#after epochs 252 (with septs/20 and batch_size 64)\n",
    "\n",
    "#model.compile(optimizer=anubi, loss=[loss_mse_select_clipped,loss_ROIsoft_crossentropy], loss_weights=[1,1]) #FOR LATE TRAINING\n",
    "model.compile(optimizer=anubi, loss=[loss_mse_select_clipped,loss_ROI_crossentropy], loss_weights=[1,1]) #FOR EARLY TRAINING\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
